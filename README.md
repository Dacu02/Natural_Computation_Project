# Natural Computation Project Framework
The following repository contains the Python source code for the execution of the experiments regarding the PSO and ABC algorithms families on a series of problem instances generated by the Generalized Numerical Benchmark Generator (GNBG). 

Full description of GNBG can be found in "D. Yazdani, M. N. Omidvar, D. Yazdani, K. Deb, and A. H. Gandomi, "GNBG: A Generalized and Configurable Benchmark Generator for Continuous Numerical Optimization," arXiv prepring arXiv:2312.07083, 2023."

## Installation
The packages required in order to run the project are listed inside the file `requirements.txt` and the python version required is at least `3.12.1`. 
To install the required packages, run:

```bash
pip install -r requirements.txt
```
## Repository structure
### `Algorithms/`
This folder contains the modules that define the classes `Algorithm`, `ParticleSwarmOptimization` and `ArtificialBeeColony`. Also, an implementation of the `DifferentialEvolution` is given, which is not guaranteed to work properly, initially used as a baseline to compare the results.
### `experiments.py`
This file contains the settings of the current run, in the version given by the repository it contains the settings used for the final comparison between the PSO and ABC. From the `utils.py` file it imports:
+ The class `Combine`, which allows to pass an array of possible values
+ A method `expand_algorithms` that uses the `Combine` class to replicate the algorithms
+ An utility method `load_from_csv` that takes as input the ranks file obtained from the `StatComparison.py` module, in order to select at most $n$ algorithms inside a specific critical difference

This file exports the literal `EXPERIMENTS` which contains the algorithm parameters configurations to run. In turn, this variable uses `_list_algorithms`, which is a dictionary of algorithm where the key is the class of problems where to run the experiments. The $-1$ key is used when those algorithms must be ran across multiple classes.

### `AggregateCSV.py`
This file is used mainly to aggregate the various file returned by the processes ran on the High Performance Cluster. It reorganizes the experiments results data in:
+ One file for each problem
+ One file for each class
+ One file for all the classes

This file structure is defined in order to be able to compare algorithms between the same problem, class and all problems.
### `Plot.py`
This utility file contains a module that plots the data of algorithms during the iteration, in order to compare them during the iterations. 
### `Framework.py`
The following module is the core of the repository, it defines the problems and the seeds on which run the experiments defined in the respective model (albeit, it is possible to pass them by command line).

In it the GNBG problem instances are initialized, and the experiments to run are distributed across the CPUs available on the machine. Since this module is executed by the HPC SLURM job array, it takes as input the job identifier in the array, and the total number of the jobs. This way, the experiments can be distributed on the nodes and then on the CPUs without overlapping.
## Execution Example
```bash
#!/bin/bash
#SBATCH --partition=defq
#SBATCH --account=did_generative_ai_336
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=76
#SBATCH --gpus-per-task=0
#SBATCH --array=0-3

# entra nella directory del progetto
cd $HOME

# inizializza conda e attiva l'ambiente
eval "$(conda shell.bash hook)"
conda activate nc_env
cd Natural_Computation_Project
python -u Framework.py $SLURM_ARRAY_TASK_ID $SLURM_ARRAY_TASK_COUNT test 2 10 18 100 200 300
```
In this script example a SLURM job is defined in which the experiments defined in `experiments.py` will be run across the problems 2, 10, 18 on the seeds 100, 200, 300. The first seed number must be bigger than 24 in order to be correctly inputted.

In order to run the same experiment on a machine without the SLURM workload manager, the last line will become:
```bash
python -u Framework.py 0 1 test 2 10 18 100 200 300
```

After the ending of the experiments, the results can, in either case, be obtained through the command:
```bash
python AggregateCSV.py test
```
A new folder will be created in `aggregated_results/aggregated_results__test/` containing `by_problem/`, `by_class/` and `all` where the results are compared.

The path of one of these folders can, in turn, be given to the `StatComparison.py` module to obtain the statistics metrics:
```bash
python StatComparison.py aggregated_results/aggregated_results__test/by_class $TEST_TYPE
```
Where `$TEST_TYPE` is `'Friedman'` is the algorithms are more than $2$, or `'Wilcoxon'` if they are exactly $2$. This command was often used to compare algorithms belonging to the same family.
### `MINIMAL_FRAMEWORK` option
The `Framework.py` module contains a `MINIMAL_FRAMEWORK` literal that sets the verbosity of the runs. If the flag is set to `True`, only the best solution given by each experiment will be saved, in order to save space, and time, on the drive. On the other hand, if it is set to `False` the best solution of each iteration will be saved in the files, thus allowing the user to generate the plots of the solution through the iterations. This can be done by running the `Plot.py` file:
```bash
python Plot.py
```
This file has an interactive input that allows to decide the experiment and algorithms to plot.
## Reproduction of the results
Running the following scripts allows the user to retrieve the same results obtained, on any machine that doesn't need the SLURM workload manager.
### ABC
### PSO
In order to run the PSO experiments, the combinations must be defined inside the `experiments.py` file. The variable `_list_algorithms` must be set to:
```python
_list_algorithms: dict[int, list[AlgorithmStructure]] = {
-1: [{
    'algorithm': ParticleSwarmOptimization,
    'args': {
        'population': Combine([11*11, 13*13]),
        'topology': Combine(['Random', 'Star', 'Torus']),
        'local_weight': Combine([.75, 1.25]),
        'global_weight': Combine([.75, 1.25]),
        'inertia': Combine([.7, .9]),
        'velocity_clamp': (-.15 * RANGE, .15 * RANGE),
        'end_inertia': Combine([.4, None])
    },
    'name': 'PSO',
}]}
```
Once this is done, it is possible to start the first step: running all the algorithms across 3 problems for each class.
### First Step
In the `Framework.py` file three literals have to be set:
```python
DEFAULT_SEEDS = [5751, 94862, 48425, 79431, 28465, 917654, 468742131, 745612, 1354987, 126879]
DEFAULT_PROBLEMS = [2, 4, 6, 10, 12, 14, 18, 20, 22]
MINIMAL_FRAMEWORK = True
```
Once this is done, it is possible to launch the experiments and execute the tests:
```bash
python Framework.py 0 1 step1
python AggregateCSV.py step1
python StatComparison aggregated_results/aggregated_results__step1/by_class Friedman
```
In the end the folder `aggregated_results/aggregated_results__step1/data_analysis` will contain the plots and the rankings that will be used in the next step, for each class, if the Friedman test successfully detects statistically significant differences between the algorithms performances. 
### Second step
In this case the `_list_algorithms` variable in `Framework.py` must be updated to only take into account the algorithms which ranking is inside the critical difference.

The `load_from_csv` takes as input:
+ The CSV file with the rankings
+ The algorithm's class
+ The critical difference

```python
_list_algorithms[1] = load_from_csv("aggregated_results/aggregated_results__step1/data_analysis/1_mean_ranks.csv", CLASS, CD[0])
_list_algorithms[2] = load_from_csv("aggregated_results/aggregated_results__step1/data_analysis/2_mean_ranks.csv", CLASS, CD[1])
_list_algorithms[3] = load_from_csv("aggregated_results/aggregated_results__step1/data_analysis/3_mean_ranks.csv", CLASS, CD[2])
```
Where the CD is obtained from the plots, in our experiments for PSO `CD=[45.38]*3` while for ABC `CD=[]`. 

After this, it is possible to define the literals of the `Frameworks.py` file:
```python
DEFAULT_SEEDS = [5751, 94862, 48425, 79431, 28465, 917654, 468742131, 745612, 1354987, 126879, 468798, 46489465, 61845421, 48512135, 323546, 3354564645, 9685412, 288484, 32626984, 15468897, 86735578, 24354843, 54768687, 656576335, 564754637, 68756436, 89775674, 356475, 634647976, 39528058, 385022085, 3590328, 395920, 654468, 455768432, 3216878, 763215897, 13549875, 1354687, 68465486]
MINIMAL_FRAMEWORK = True
```
The problems are omitted, since passed through the command line, this allows for three different execution:
```bash
python Framework.py 0 1 step2_class1 2 4 6
python Framework.py 0 1 step2_class2 10 12 14
python Framework.py 0 1 step2_class3 18 20 22

python AggregateCSV.py step2_class1
python StatComparison aggregated_results/aggregated_results__step2_class1/by_class Friedman

python AggregateCSV.py step2_class2
python StatComparison aggregated_results/aggregated_results__step2_class2/by_class Friedman

python AggregateCSV.py step2_class3
python StatComparison aggregated_results/aggregated_results__step2_class3/by_class Friedman
```
If the Friedman test is successful the Nemenyi critical difference of Nemenyi will be computed and plotted, the results will be in three different folders this time: `aggregated_results__step2_class{i}/data_analysis` for each class `i`.
### Third step
The same previous operations are done in order to retrieve the subset of algorithms on which the experiments are going to be done. In the `experiments.py` file the variable is rewritten as:
```python
_list_algorithms[1] = load_from_csv("aggregated_results/aggregated_results__step2_class1/data_analysis/1_mean_ranks.csv", CLASS, CD[0])
_list_algorithms[2] = load_from_csv("aggregated_results/aggregated_results__step2_class2/data_analysis/2_mean_ranks.csv", CLASS, CD[1])
_list_algorithms[3] = load_from_csv("aggregated_results/aggregated_results__step2_class3/data_analysis/3_mean_ranks.csv", CLASS, CD[2])
```
Where, from the results, we set `CD=[12.59,15.07,9.66]` for PSO and `CD=[]` for ABC.

Now, the new set of seeds must be passed to the `Framework.py` file:
```python
SEEDS = [24253325, 53567257, 53436736, 48397383, 375897583, 72465234, 546326235, 532532646, 734625466, 235464333, 46234664, 46334225, 352, 3252532, 32535233, 235252, 323523, 903325, 8465486, 76654654, 4984654, 8946549, 654, 5442, 3158468, 6546123, 8648200, 503210, 6498005, 6546, 6254, 865484, 654789, 5965, 6547812, 4568723, 64789, 128546, 3133252, 64823497654, 96458, 1645, 4585446, 4578132, 15866415, 49756112, 50558657, 68462335, 841365444, 425643, 9083532, 72488574, 9520413, 90320213, 62390235, 360235, 3529032, 90232235, 92358003]
MINIMAL_FRAMEWORK = True
```
Like before the problems are given to the python file through the command line, this time with six problems per class:

```bash
python Framework.py 0 1 step3_class1 1 2 3 4 5 6
python Framework.py 0 1 step3_class2 9 10 11 12 13 14
python Framework.py 0 1 step3_class3 17 18 19 20 21 22

python AggregateCSV.py step3_class1
python StatComparison aggregated_results/aggregated_results__step3_class1/by_class Friedman

python AggregateCSV.py step3_class2
python StatComparison aggregated_results/aggregated_results__step3_class2/by_class Friedman

python AggregateCSV.py step3_class3
python StatComparison aggregated_results/aggregated_results__step3_class3/by_class Friedman
```
