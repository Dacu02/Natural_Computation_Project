\documentclass[11pt]{article}

% \usepackage[utf-8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{multirow}
\usepackage{float}
\usepackage{setspace}
\usepackage{longtable}
\usepackage[style=numeric, backend=bibtex]{biblatex}
\addbibresource{Documentation.bib}
\usepackage{xcolor}
%\pagecolor[rgb]{0.05,0.05,0.05}
%\color[rgb]{0.9,0.9,0.9}

\geometry{margin=1in}
\onehalfspacing

\title{Natural Computation Project Documentation}
\author{Noemi Biancamano \and Davide D'Acunto}
\date{}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subcaption}

\lstset{
    language=Python,
        basicstyle=\ttfamily\small,
        keywordstyle=\color{blue},
        commentstyle=\color{gray},
        stringstyle=\color{red},
        breaklines=true,
        showstringspaces=false,
        tabsize=4,
        emph={Combine,ParticleSwarmOptimization,ArtificialBeeColony},
        emphstyle=\color{purple}
}
\begin{document}

\maketitle

\begin{abstract}
\noindent The following document compares the performance of different algorithms implemented for solving problems instances generated by the Generalized Numerical Benchmark Generator (GNBG). Two main families of the swarm intelligence algorithms are considered: Particle Swarm Optimization (PSO) and Artificial Bee Colony (ABC).
\end{abstract}
\tableofcontents
\newpage
\section{Introduction}
\subsection{Generalized Numerical Benchmark Generator}
The Generalized Numerical Benchmark Generator (GNBG) is a set of benchmark functions used for evaluating the performance of optimization algorithms. These 24 functions are grouped into three categories of 8 problems each:
\begin{itemize}
    \item \textbf{Unimodal functions}: these functions have a single global minimum, often having a landscape smoother than the other categories and sometimes even convex
    \begin{itemize}
        \item This class evaluates how well the algorithm obtains information from the landscape, and how well it exploits this information to converge to the global minimum, since in this case there are not local minima to deceive the algorithm.
    \end{itemize}
    \item \textbf{Single component Multimodal functions}: these functions have multiple local minima, making them more challenging for optimization algorithms since their ability to deceive the algorithms
    \begin{itemize}
        \item This class evaluates how well the algorithm can escape from local minima, and how well it can explore the search space to find the global minimum.
    \end{itemize}
    \item \textbf{Multi component Multimodal functions}: these functions have multiple local minima, but they are more complex than the single component multimodal functions, since they are composed of multiple components, which make the landscape unpredictable and more difficult to navigate
    \begin{itemize}
        \item This class evaluates how well the algorithm can navigate through a complex landscape, and how well it can balance exploration and exploitation to find the global minimum.
    \end{itemize}
    \end{itemize}
\subsection{Particle Swarm Optimization}
The Particle Swarm Optimization (PSO) is a population-based optimization in which a swarm of particles (potential solutions) moves through the search space to find the optimal solution. Each particle adjusts its position based on its own experience and the experience of neighboring particles.
\\[1em]
The update rule for the velocity and position of the $i$-th particle on iteration $t$ in PSO with inertia \cite{pso} weight $w$ is given by:
\begin{align*}
\overset{\rightarrow}{v_i}(t+1) &= w \cdot \overset{\rightarrow}{v_i}(t) + \mathcal U(0,c_1) \cdot (\overset{\rightarrow}{p_i} - \overset{\rightarrow}{x_i}(t)) + \mathcal U(0,c_2) \cdot (\overset{\rightarrow}{g_i} - \overset{\rightarrow}{x_i}(t)) \\
\overset{\rightarrow}{x_i}(t+1) &= \overset{\rightarrow}{x_i}(t) + \overset{\rightarrow}{v_i}(t+1)
\end{align*}
In the subsequent sections, each term of the update rule will be explained in detail, along with the parameters involved and their influence on the algorithm's performance.
\subsubsection{Population \& Topology}
A higher population size allows for a more extensive exploration of the search space, but decreases the number of iterations, meanwhile a smaller population size allows for more iterations on fewer particles. The topology affects how particles share information: it oscillates between a global topology, where all particles connected between each other, and a local topology, where particles only interact with few neighbors. 
\\[1em]
The topologies considered in this project are:
\begin{itemize}
    \item \textbf{Random Topology}: particles are connected randomly to $k$ other particles, where $k$ is set to 5 in the experiments. 
    \item \textbf{Star Topology}: all particles are connected to a central particle, which shares the best position to all other particles.
    \item \textbf{Torus Topology}: this topology is a special case of the Von Neumann topology, in which particles are arranged in a 2D grid, and each particle is connected to its four immediate neighbors (up, down, left, right). The Torus topology wraps around, meaning that the particles on the edges are connected to those on the opposite edge, creating a toroidal structure.
\end{itemize}
Since a Torus topology is adopted, the chosen population sizes will be perfect squares, in order to create a square grid of particles. The population sizes considered in the experiments are $121$ and $169$. 
\subsubsection{Position \& Velocity}
The position of each particle represents a potential solution to the problem, while the velocity determines how the particle moves through the search space, with respect to the other parameters. Since the solution space is not $\mathbb R^d$, they must be limited to the range of the problem:
\begin{itemize}
    \item \textbf{Position}: the position of each particle is limited to the range of the problem, which is $[-100, 100]$ for all dimensions. If a particle is going to exceed a boundary, its velocity is shrunk by a factor such that the particle falls on the boundary.
    \item \textbf{Velocity}: the velocity of each particle is limited to a certain range, if the velocity exceeds the maximum (or minimum) velocity, it is clamped to the maximum (or minimum) velocity. The range of the velocity is set to $\pm0.15$ times the difference between the maximum and minimum positions: $\left[-30, 30\right]$. It is important to note that the velocity is independently limited for each dimension. This customization was suggested in \cite{clamp}
\end{itemize}
\subsubsection{Cognitive and Social Weights}
The cognitive and social weights influence respectively the particle's tendency to return to its own best position and the tendency to move towards the best position found by its neighbors. A higher cognitive weight encourages particles to explore the search space more independently, while a higher social weight encourages particles to converge towards the best solution found by the swarm. The values of the cognitive and social weights considered are all combinations of $c_1$ and $c_2$ in $\{0.75, 1.25\}$.
\subsubsection{Inertia}
The inertia weight controls the influence of the previous velocity on the current velocity. It is somewhat similar to the temperature of the particles. In this case other than the standard fixed inertia weight, a linearly decreasing inertia weight is also considered. The inertia weights start from $0.9$ or $0.7$ and eventually if enabled decreases to $0.4$ through the iterations.
\newpage
\subsection{Artificial Bee Colony}
Artificial Bee Colony (ABC) is a swarm intelligence algorithm inspired by the foraging behavior of honey bees.  
The algorithm reproduces how the bees search for food sources, and how they share information about the quality of the food sources with other bees and the process of collecting nectar from the food sources. The ABC algorithm is used for solving optimization problems, even with non linear, multimodal and high dimensional search spaces.

\subsubsection{Bees Types}
The analogy with the behavior of real bees is reflected in the algorithm through the division of the population into three types of bees: employed bees, onlooker bees, and scout bees. Each type of bee has a specific role in the search process:
\begin{itemize}
    \item \textbf{Employed Bees}: these bees are responsible for exploring the search space and exploiting the food sources they have found. Each employed bee is associated with a specific food source, and it shares information about the quality of that food source with the onlooker bees. The employed bees also perform a local search around their associated food source to find better solutions.
    The food sources correspond to potential solutions to the optimization problem, and the quality of a food source is determined by the objective function value of the corresponding solution. 
    \item \textbf{Onlooker Bees}: these bees are responsible for selecting food sources based on the information shared by the employed bees. They choose food sources with a probability proportional to their quality, and they also perform a local search around the selected food source to find better solutions.
    \item \textbf{Scout Bees}: these bees are responsible for exploring the search space randomly to find new food sources. If a food source is abandoned by the employed bees (i.e., it has not been improved for a certain number of iterations), a scout bee is sent to search for a new food source randomly in the search space.
\end{itemize}
Sharing information among the bees occurs in a specific region, called dancing area or hive, where information about food sources are shared through a process called waggle dance. The dancing is proportional to the quality of the food source, so that better food sources are more likely to be selected by the onlooker bees. 
At the beginning, a potential forager starts as unemployed, having no information about the food sources. Than we have to possible outcomes: can become a scout bee and start searching for food sources near the hive, or it can become a recruiter bee, and start following the waggle dance of the employed bees to find a food source. 
After finding a food source, the bee becomes an employed bee, and it starts exploiting the food source, sharing information about its quality with the onlooker bees. After unloading the nectar collected from the food source, the employed bee becomes unemployed again, and it can start searching for new food sources as a scout bee, or it can start following the waggle dance of the employed bees to find a food source as a recruiter bee. 


\subsubsection{Explanation of the analogy}
Now that we know how the algorithm works, we can explain the analogy with the behavior of real bees in more detail. 
In our analogy, the food sources correspond to potential solutions to the optimization problem, and the quality of a food source is determined by the objective function value of the corresponding solution. 

The ABC algorithm can be synthesized as follows:

\paragraph{Initialization:}

a set of candidate solution(employed bees) is randomly generated in the search space, and their quality is evaluated by the objective function. The best solution found so far is stored as the current global best.
So we generate a population of size $N$ of $\{ x_i \}_{i=1}^N$ as follows: 
$$ x_{i,j} = x_{min,j} + rand(0,1) \cdot (x_{max,j} - x_{min,j}),\quad i=1,\ldots,N,\quad j=1,\ldots,D$$
Where $D$ is the dimensionality of the problem, and $x_{min,j}$ and $x_{max,j}$ are the lower and upper bounds of the search space, respectively. 



\paragraph{Employed Bee Phase:}
during the search process, employed bees apply a perturbation to the current solution in order to explore the neighborhood of the current solution, and they evaluate the new solution. Each solution is evaluated by the objective function, and if the new solution is better than the current solution, the employed bee updates its position to the new solution and shares this information with the onlooker bees.(making the solution a global best)
Startin from the current solution $x_i$, an employed bee generates a new solution $v_i$ in the neighborhood of $x_i$ as follows:
$$ v_{i,j} = x_{i,j} + \phi_{i,j} \cdot (x_{i,j} - x_{k,j}),\quad k \neq i$$
\indent where:
\begin{itemize}
    \item $x_K$ is a randomly selected solution from the population, different from $x_i$:
    \item $\phi_{i,j}$ is a random number uniformly distributed in the range $[-1, 1]$;
    \item $j$ is a randomly selected dimension index;
\end{itemize}
Than the new solution $v_i$ is evaluated by the objective function comparing it with the current solution $x_i$. For minimization problems the fiteness of a solution is calculated as follows:
$$ fit_i = \begin{cases}\dfrac{1}{1 + f(x_i)}, & \text{if } f(x_i) \geq 0 \\[10pt]1 + |f(x_i)|, & \text{otherwise}\end{cases} $$
Where $f(x_i)$ is the objective function value. 

\paragraph{Onlooker Bee Phase:}
onlooker bees select a food source based on the information shared by the employed bees, and they apply a perturbation to the selected solution in order to explore the neighborhood of the selected solution, and they evaluate the new solution. If the new solution is better than the selected solution, the onlooker bee updates its position to the new solution and shares this information with the employed bees. (taking the place of the employed bee that found the new solution)
In these phase onlooker bees select a solution $x_i$ with a probability proportional to its fitness:
$$ p_i = \dfrac{fit_i}{\sum_{j=1}^N fit_j} $$
In the original implementation of the algorithm, onlooker bees select a solution by using a roulette wheel selection based on the probabilities $p_i$, probabilist selection can be performed also using ranked selection or tournamente selection.
\paragraph{Scout Bee Phase:}
if an employed bee has not improved its solution for a certain number of iterations, it becomes a scout bee and randomly searches for a new solution in the search space. 

\subsubsection{Popoluation \& Bees Strategy}
In the following sections, the parameters of the ABC algorithm will be explained in detail, along with their influence on the algorithm's performance, putting apart parameters depending on the choosen implementation of the algorithm. 
\\[1em]
The first parameter common in all the optimization algorithms is the population size, which in this case corresponds to the number of bees. A higher population size allows for a more extensive exploration of the search space, while a smaller population size may lead to premature convergence. The population size is typically set to a value that balances exploration and exploitation effectively.
\noindent \\
The number of employed bees is typically set to half of the total population size, while the number of onlooker bees is set to the other half. This allows for a balance between exploration and exploitation, as the employed bees focus on exploiting the current solutions, while the onlooker bees focus on exploring new solutions based on the information shared by the employed bees. The number of scout bees is typically set to a small value, to allow for some random exploration without overwhelming the search process.

\subsubsection{Limit}

The second parameter is the limit, which determines the number of iterations an employed bee can go without improving its solution before it becomes a scout bee. A smaller limit encourages more exploration, while a larger limit allows for more exploitation of the current solutions. The limit is typically set to a value that allows for sufficient exploration without causing excessive random search.






\section{Methods}
In order to compare the performance of the algorithms, statistical tests are performed on the results obtained from the experiments ran on a subset of the problem instances. Based on the type of comparison, i.e. whether many algorithms of the same family are compared among themselves, or whether one algorithm from each family is compared, different tests are performed.
\\[1em]
The methods used for the comparisons will be explained in detail in the following subsections, along with the reason behind their choice. In the next section then the implementation of the methods will be swiftly explained, and finally their planning and execution will be described. The notions are taken from \cite{demsar}.
\subsection{Multiple algorithms of one family comparison}
This case considers the comparison of multiple algorithms of the same family: algorithms belonging to the same family (PSO or ABC) but differing in the parameters used. In this case it is assumed that the algorithms are more than two, and that the results obtained from the experiments are not normally distributed. For this reason, non-parametric tests are performed.
\subsubsection{Friedman Test}
The Friedman test is among the most widely used non-parametric tests for comparing multiple algorithms on multiple datasets, where in this case the dataset consists of the results obtained from the experiments. Its first step is to rank the algorithms on each problem, assigning the rank $1$ to the best performing algorithm, $2$ to the second best, and so on. If two or more algorithms have the same performance, they are assigned the average rank (e.g., if two algorithms are tied for first place, they would both receive a rank of $\dfrac{1+2}2=1.5$).
\\[1em]
After ranking the algorithms on each problem, the mean rank of each algorithm across the considered problems is calculated. This mean rank allows to determine an estimator of the performance of each algorithm which is not affected by the different scales of the results obtained from the different problems. In order to determine whether the observed differences in mean ranks are statistically significant, the Friedman test statistic is calculated: $$Q=\dfrac{12n}{k(k+1)}\sum_{j=1}^{k}\left(\bar r_j-\dfrac{k+1}{2}\right)^2,\qquad \bar r_j=\dfrac1n\sum_{i=1}^n r_{ij},\qquad Q\sim\chi_{(k-1)}^2$$
In the formula, $n$ is the number of problems, $k$ is the number of algorithms, and $\bar r_j$ is the mean rank of the $j$-th algorithm. In order to understand the formula, it is important to explain the intuition behind it. First, the rank $r_{ij}$ itself is distributed accordingly to a discrete uniform distribution. Since under the null hypothesis, all algorithms are expected to perform equally, so they share the same mean rank: $$r_{ij}\sim\mathcal U\left\{1,\ldots,k\right\},\quad \mathbb E\left[r_{ij}\right]=\dfrac{k+1}{2}\overset{H_0}=\mathbb E\left[\bar r_j\right],\quad Var\left[r_{ij}\right]=\dfrac{k^2-1}{12}$$
From which the variance of the mean ranks can be derived: $$Var\left[\bar r_j\right]=Var\left[\dfrac1n\sum_{i=1}^n r_{ij}\right]=\dfrac 1{n^2}\cdot n\dfrac{k^2-1}{12}=\dfrac{k^2-1}{12n}$$
Now, the Friedman test purpose is to measure the deviation of the observed mean ranks from the expected mean rank under the null hypothesis: $$S=\sum_{j=1}^k\left(\bar r_j-\dfrac{k+1}2\right)^2$$
This deviation must then be normalized by the rank variance, in order to be comparable. The Friedman test statistic also introduces one degree of freedom correction because of the covariance between the ranks. In the end, the formula takes the form of a chi-square distribution with $k-1$ degrees of freedom, as shown above. 
\\[1em]
The more the observed mean ranks deviate from the expected mean rank, the higher the value of $Q$, and the more likely it is to reject the null hypothesis. Also, $k$ and $n$ have an influence on the value of $Q$ too: as the number of algorithms $k$ increases, the statistics decreases, while if the number of experiments $n$ increases, the statistic increases.
\subsubsection{Nemenyi post-hoc}
The Friedman test determines whether there are statistically significant differences among the algorithms, but it doesn't specify which algorithms differ. In order to retrieve this information, the Nemenyi post-hoc test is performed. This test purpose is to compare all pairs of algorithms, and determine whether the null hypothesis of equal performance can be rejected for each pair. 
\\[1em]
In order to determine whether the performance of two algorithms $i$ and $j$ is significantly different, the variance of the difference between their mean ranks is calculated: $$Var\left[\bar r_i-\bar r_j\right]=Var\left[\bar r_i\right]+Var\left[\bar r_j\right]-2\ Cov\left[\bar r_i,\bar r_j\right]$$
As aforementioned, the ranks are not independent between themselves: if one algorithm has a high rank on a problem, another algorithm must have a low rank on the same problem. In order to retrieve the covariance between the ranks, some steps must be taken. Knowing that the sum of the ranks on each problem is constant: $$\sum_{j=1}^k r_{ij} = \frac{k(k+1)}{2}\Longrightarrow0=Var\left[\sum_{j=1}^k r_{ij}\right]=\sum_{j=1}^kVar\left[r_{ij}\right]+2\sum_{1\le j<\ell\le k}Cov(r_{ij},r_{i\ell}),\qquad\forall i$$
Solving for the covariance, knowing the variance and knowing that all the covariances are equal, it can be derived that: $$Cov(r_{ij},r_{i\ell})=-\dfrac{k+1}{12},\qquad 1\le j<\ell\le k,\quad\forall i$$
From which the variance of the difference between the mean ranks can be derived: $$Var\left[\bar r_i-\bar r_j\right]=\dfrac{k^2-1}{12n}+\dfrac{k^2-1}{12n}+2\cdot\dfrac{k+1}{12n}=\dfrac{k(k+1)}{6n}$$
This would be sufficient to perform a z-test on the difference between the mean ranks of the two algorithms, but since multiple comparisons are performed, a correction is done by introducing the critical value of the Studentized range distribution $q_\alpha$, which is defined as: $$q=\dfrac{\max\left(Z_i\right)-\min\left(Z_i\right)}{SE\left(Z_i\right)}\qquad q\sim q_{k,\nu}$$
Where $k$ is the number of algorithms, $\nu$ is the degrees of freedom. Since the variances of the mean ranks are known, it is assumed that $\nu=\infty$, meanwhile for the same reason mentioned before, one degree of freedom is detracted from $k$, so the critical value is $q_\alpha=q_{k-1,\infty}(\alpha)$.
\\[1em]
In the end, the performance of two algorithms $i$ and $j$ is significantly different if the absolute difference between their mean ranks is greater than the critical difference: $$|\bar r_i-\bar r_j|>q_\alpha\sqrt{Var\left[\bar r_i-\bar r_j\right]}=q_\alpha\sqrt{\dfrac{k(k+1)}{6n}}=CD$$
This quantity is called the critical difference (CD), and it represents the minimum difference between the mean ranks of two algorithms for their performance to be considered significantly different. As the number of algorithms $k$ increases, the critical difference increases, so it is harder to find significant differences between the algorithms. On the other hand, as the number of problems $n$ increases, the critical difference decreases. The $\alpha$ level also has an influence on the critical difference: as $\alpha$ decreases, the critical value $q_\alpha$ increases.
\subsection{One algorithm for each family comparison}
This case considers the comparison of one algorithm from each family: one PSO and one ABC. In this case, it is assumed that the results obtained from the experiments are not normally distributed. For this reason, non-parametric tests are performed.
\noindent \\
In the next sections, the Wilcoxon test is performed to compare the performance of the two algorithms on each problem, and then a post-hoc comparison is performed to determine whether the observed differences are statistically significant. Its used as an alternative to the paired t-test when the normality assumption is not met.
\subsubsection{Wilcoxon Test}
The Wilcoxon signed-rank test is a non-parametric test used to compare two related samples, in this case the results obtained from the two algorithms on the same problem instances. The test is based on the differences between the paired observations, and it assesses whether the median of these differences is significantly different from zero. 
\\
Considering two algorithms $A$ and $B$, the first step is to calculate the differences between the paired observations: 
$$d_i = A_i - B_i$$ 
where $A_i$ and $B_i$ are the results obtained from the two algorithms on the $i$-th problem instance. Then, the absolute values of these differences are ranked, ignoring any differences that are equal to zero. If there are ties in the absolute differences, they are assigned the average rank (as Friedman ranking). Next, the ranks are assigned a positive or negative sign based on the original differences: if $d_i$ is positive, the rank is positive; if $d_i$ is negative, the rank is negative. 
Under the null hypothesis that there is no difference between the two algorithms, the sum of the positive ranks and the sum of the negative ranks should be approximately equal. The test statistic is then calculated as the smaller of these two sums:
$$W = \min\left(\sum_{d_i > 0} r_i + \dfrac{1}{2} \sum_{d_i = 0} r_i, \sum_{d_i < 0} r_i + \dfrac{1}{2} \sum_{d_i = 0} r_i\right)$$
Where $r_i$ is the rank of the absolute difference $|d_i|$, note that ranks of $d_i = 0$ are split equally between the positive and negative sums. \\
The p-value can be computed considering that under the null hypothesis, all the configuration are equally likely, with probability ${2^{-n}}$, where $n$ is the number of non-zero differences. 
In order to obtain the probability all the configurations of signs for the ranks are counted, and the number of them that lead to a sum of positive ranks (or negative ranks) less than or equal to the observed test statistic $W$ is determined. The p-value is then calculated as:
$$p = \dfrac{\text{\#configurations with } W' \leq W}{2^n}$$
Where $W'$ is the test statistic calculated for each configuration of signs. If the p-value is less than the chosen significance level $\alpha$, the null hypothesis is rejected, indicating that there is a statistically significant difference between the two algorithms. 
For larger sample sizes it is assumed that the distribution of the test statistic $W$ can be approximated by a normal distribution, allowing for the use of a z-test to calculate the p-value. In this case, the test statistic is standardized as follows:
$$Z = \dfrac{W - E[W^+]}{\sqrt{Var(W^+)}}$$
Where $E[W^+]$ and $Var(W^+)$ are the mean and variance of the test
statistic $W$ under the null hypothesis, which can be calculated as:
$$E[W^+] = \dfrac{n(n+1)}{4},\quad Var(W^+) = \dfrac{n(n+1)(2n+1)}{24}$$
The p-value can then be calculated using the standard normal distribution.
\section{Execution}
In the following section, the execution procedure is thoroughly explained, along with the machines used for the experiments.
\\[1em]
Regarding the machines, the experiments were run on the High Performance Computing (HPC) cluster of the University of Salerno, which is composed of $16$ nodes executing Ubuntu Linux $6.8$ mounting each $72$ CPU cores, for a total of $1152$ cores. For this reason, the experiments were parallelized: for each node a process is executed, each one initializes $72$ threads, so that each thread runs an experiment on a different problem instance. In order to distribute the load among the nodes, the experiments are distributed with a job array. It allows to submit a single job to the cluster, which will then replicate itself on each node, but with a different index, so that each node will execute a different subset of the experiments. The results obtained from the experiments are then collected, aggregated, and then analyzed with the statistical tests.
\\[1em]
In order to take into account the maximum execution time of the experiments on the nodes allowed by the cluster policies, the mean time to execute an experiment is estimated to last less than 20 minutes. Since the number of cpus is $1152$, and the maximum time allowed for a job is $9$ hours, the maximum amount of experiments that can be executed in parallel is $1152\cdot\dfrac{9\cdot60}{20}=31104$. This number far exceeds the amount of experiments that will be executed, and in order to avoid overloading the cluster, the experiments are distributed through time on fewer nodes. 
\subsection{Framework}
The framework of the project is implemented in Python, and it is composed of various modules that represent the different components of the algorithms, which were developed independently from each other, in order to split the execution part from the data retrieval and analysis part.
\subsubsection{\texttt{Algorithm.py}}
Through this module, the abstract class that an algorithm must implement is defined. This way by managing the algorithms through a common interface, the execution of the experiments is simplified, and the results can be easily retrieved and analyzed by defining a common format for the results. This module also defines the common parameters of the algorithms, such as the population size, the verbosity level, the number of generations and the seed.
\subsubsection{\texttt{ParticleSwarmOptimization.py}}
The Particle Swarm Optimization (PSO) algorithm is implemented in this module through the Pyswarms library \cite{pyswarms} and the \texttt{Algorithm} abstract class. This library was chosen because it provided a flexible implementation of the PSO algorithm, which exposed the parameters needed for the experiments, the linear decay of the parameters, and the possibility to implement custom topologies. 
\\[1em]
The first part of the file contains literals describing:
\begin{itemize}
    \item The strategy to adopt when a particle is going to exceed the boundaries of the problem
    \item The strategy to adopt when a particle is going to exceed the velocity limits
    \item The strategy to vary the parameters of the algorithm through the iterations
\end{itemize}
Then, the Torus topology is implemented since not available in the library, and finally the PSO algorithm class is implemented through the library, which constructor allows to set the parameters previously mentioned.
\subsubsection{\texttt{ArtificialBeeColony.py}}
The Artificial Bee Colony (ABC) algorithm is implemented in this module through the beeoptmal library \cite{beeoptimal} and the \texttt{Algorithm} abstract class. This library was chosen because it provided a flexible implementation of the ABC algorithm, which exposed the parameters needed for the experiments, and the possibility to implement custom strategies for the employed bees, onlooker bees, and scout bees.

The implementation of the ABC algorithm proposed by beeoptimal offers the possibility to customize the mutation strategy of the employed bees, the selection strategy of the onlooker bees: 

The packages offers the possibility to customize the inizialitaion strategy of the employed bees, the mutation strategy of the employed bees, the selection strategy of the onlooker bees, and the strategy to determine when a food source is abandoned by the employed bees. 

The inizialization strategy of the employed bees available in the library are:
\begin{itemize}
    \item \textbf{Random Initialization}: the employed bees are initialized randomly in the search space, as described in the previous sections.
    \item \textbf{Cahotic Initialization}: the employed bees are initialized using a chaotic sequence. \cite{cahotic}
\end{itemize}
The mutation strategy of the employed bees available in the library are:
\begin{itemize}
    \item \textbf{Standard }: the employed bees generate new solutions by perturbing their current solution, as the standard ABC algorithm does.
    \item \textbf{Best }: inspired by Differential Evolution, the employed bees generate new solutions by perturbing the current best solution found by the swarm.
    \item \textbf{Modified }: the employed bees generate new solutions by perturbing their current solution, but the perturbation is multi-dimensional, thourgh a vector of random numbers and mutation rate, that is the probability that each dimension is perturbed.
    \item \textbf{Directed}: this mutation incorporates information about the direction of the search, for each dimension is added a direction information. For better understandind of this strategy, please refer to the documentation of the library \cite{beeoptimal}.
\end{itemize}
For the selection strategy we propose a custom implementation of the mutation strategy, even if they will be no-performing. 
The selection strategy implemented are based on the idea of the paper \cite{iABC}, and consist of two variants of Intelligent ABC algorithms: \textbf{iABC} and \textbf{iABC-block}.
\\[1em]
\textbf{Intelligent Donor Selection:} Both variants use the function \texttt{get\_iabc\_donor()} which implements an adaptive donor selection mechanism based on fitness-relative comparisons:
\begin{enumerate}
    \item Calculate the average fitness $\bar{f}$ across the entire population.
    \item For a bee with current fitness $f_i$:
    \begin{itemize}
        \item If $f_i < \bar{f}$ (better than average): select donor from all bees except the current one.
        \item If $f_i \geq \bar{f}$ (worse than or equal to average): select donor only from bees with strictly better fitness ($f_j < f_i$).
        \item If no better bees exist: fallback to selecting from all bees except current.
    \end{itemize}
\end{enumerate}
This strategy promotes intelligent exploration by ensuring each bee is matched with appropriately challenging donors based on its relative fitness position.
\\[1em]
\textbf{iABC Mutation Strategy:} Extends the Modified ABC approach with intelligent donor selection. It modifies multiple dimensions using:
\begin{itemize}
    \item A mutation mask per dimension: $M_i = \mathcal{U}(0,1) < \text{mutation rate}$ 
    \item A single scaling factor $\phi \in [-\text{sf}, \text{sf}]$ applied to all mutated dimensions.
    \item Update rule: $x_{\text{new}}[i] = x[i] + \phi \cdot (x[i] - x_{\text{donor}}[i])$ for each dimension where $M_i = \text{True}$.
\end{itemize}
\noindent
\textbf{iABC-block Mutation Strategy:} Introduces block-structured mutations to handle correlated variables:
\begin{itemize}
    \item Block size calculation: $b = \max(3, \lfloor 0.2 \cdot D \rfloor)$ where $D$ is problem dimensionality.
    \item Randomly select $b$ dimensions to form a mutation block.
    \item Generate independent scaling factors $\phi_i \in [-\text{sf}, \text{sf}]$ for each dimension in the block.
    \item Apply per-dimension mutation mask: $M_i = \mathcal{U}(0,1) < \text{mr}$.
    \item Update only selected block dimensions: $x_{\text{new}}[i] = x[i] + \phi_i \cdot (x[i] - x_{\text{donor}}[i])$ where $M_i = \text{True}$.
\end{itemize}
The block-based approach promotes coordinated mutations of related variables, potentially improving convergence on separable and partially separable functions.
\\[1em]
\textbf{Implementation Details:} These variants are enabled via runtime patching through the \texttt{patch\_iabc()} function, which dynamically extends the base ABC algorithm's \texttt{get\_candidate\_neighbor\_} method without modifying the underlying library code. This allows seamless integration of iABC and iABC-block into the experimental framework alongside standard mutation strategies.

\subsubsection{\texttt{Framework.py}}
The \texttt{Framework} module is responsible for executing and saving the various experiments ran on the different problem instances with the different algorithms and seeds. It allows to execute the experiments in parallel, which results are then saved in a common CSV format, in order to be easily retrieved and analyzed by the other modules.
\subsubsection{\texttt{AggregateCSV.py}}
The following module is responsible for aggregating the results obtained from the various processes and saving them into CSV files: one for each problem, one for each problem class, and one for the overall results. Specifically, the files report results per experiment for individual problems, per experiment and problem within each class, and per experiment, problem, and class for the overall summary.
\subsubsection{\texttt{StatComparison.py}}
This module is responsible for performing the statistical tests on the results obtained from the experiments, in the format saved by the \texttt{AggregateCSV} module. It implements the Friedman test, the Nemenyi post-hoc test, and the Wilcoxon test, which are used for the comparisons of multiple algorithms. 
\subsection{Parameters combinations}
All the combinations of parameters of the algorithms are written inside the configuration file \texttt{experiments.py}, which is in turn read by the \texttt{Framework} module to execute the experiments. The various parameters combinations are chosen accordingly to the reasoning explained in the previous sections. They make use of a custom class \texttt{Combine} which allows to easily generate all the combinations of the parameters specified in the configuration file. 
\subsubsection{PSO}
By the reasoning explained in the previous sections, the parameters combinations of the PSO algorithm are as follows:
\begin{lstlisting}
[{
    'algorithm': ParticleSwarmOptimization,
    'args': {
        'population': Combine([11*11, 13*13]),
        'topology': Combine(['Random', 'Star', 'Torus']),
        'local_weight': Combine([.75, 1.25]),
        'global_weight': Combine([.75, 1.25]),
        'inertia': Combine([.7, .9]),
        'velocity_clamp': (-.15 * RANGE, .15 * RANGE),
        'end_inertia': Combine([.4, None])
    },
    'name': 'PSO',
}]
\end{lstlisting}
Where \texttt{end\_inertia} is the last value of the inertia weight in case of a linearly decreasing inertia, and \texttt{velocity\_clamp} is the range of the velocity. The other parameters are self-explanatory. The parameters will be referred as such in the following sections.
\\[1em]
Since there are:
\begin{itemize}
    \item Two population sizes
    \item Three topologies
    \item Two local weights
    \item Two global weights
    \item Two inertia weights
    \item Two end inertia weights (including the case of fixed inertia)
    \item One velocity clamp
\end{itemize}
The total number of combinations of parameters for the PSO algorithm is $2\cdot3\cdot2\cdot2\cdot2\cdot2\cdot1=96$, meaning there are $96$ different algorithms to be compared for the PSO family.
\subsubsection{ABC}
The parameters combinations of the ABC algorithm are as follows:
\begin{lstlisting}
[{
    'algorithm': ArtificialBeeColony,
    'args': {
        'population': 100, 
        'max_scouts': Combine([5, 10,20]),
        'limit': Combine([2000, 2500, 3000]),
        'selection_strategy':  'Tournament' 
        'mutation_strategy': Combine(['StandardABC', 'ModifiedABC', 
            'DirectedABC', 'iABC']),
        'initialization_strategy': 'random',
        'tournament_size': Combine([3, 5, 7])
    },
    'name': 'ABC',
}]
\end{lstlisting}
there are:
\begin{itemize}
    \item One population size
    \item Three values for the maximum number of scouts
    \item Three values for the limit
    \item One selection strategy (the custom tournament selection strategy)
    \item Four mutation strategies
    \item One initialization strategy (the random initialization strategy)
    \item Three values for the tournament size of the selection strategy
\end{itemize}
The total number of combinations of parameters for the ABC algorithm is $1\cdot3\cdot3\cdot1\cdot4\cdot1\cdot3=108$, meaning there are $108$ different algorithms to be compared for the ABC family.

\subsection{Planning}
Both the PSO and ABC algorithms undergo the same procedure, since they have almost the same amount of parameters combinations to be tested. The procedure is divided into five main steps. The first step consists in testing all the algorithms on some problems of each class in order to split the algorithms into three subsets: one for each class. The second and third steps consist in filtering, and are executed separately for each class. The continuous filtering helps to obtain more robust results without increasing the number of algorithms to be compared and experiments to be executed.
\\[1em]
In order to have a first type error of $\alpha=0.1$ on the global comparison of the algorithms on each family, for every step the significance level is set to $0.025$, since they will sum up across the steps. The last step is excluded by this procedure, since it is an independent final comparison, and for this reason the significance level is set to $0.05$.
\subsubsection{All algorithms on each class}
All the possible parameters combinations of the algorithms (which will be referred to as "algorithms" for simplicity) are tested on three problems for each class, and for each problem $10$ different seeds are used, in order to obtain $10$ different results (which will be referred as "experiments"). This procedure is applied to all the algorithms of the same family, for each family separately. The results obtained from these experiments are then aggregated by class, which allows to compare the performance of the algorithms with the Friedman ranking.
\\[1em]
For each algorithm the mean rank across the problems of the same class is calculated: $$\bar r_{jc}=\dfrac1{n\cdot m}\sum_{\substack{i=1,\ldots,n\\k=1,\ldots,m}}r_{ijkc}$$
Where $r_{ijkc}$ is the rank of the $j$-th algorithm on the $i$-th problem for the $k$-th seed in the $c$-th class, $n=3$ is the number of problems in the class, and $m=10$ is the number of seeds. Thus, the mean ranks are calculated by averaging the ranks obtained from the different experiments of all the problems and seeds in one class, in order to obtain a more robust estimator of the performance of each algorithm on the class rather than on a single problem.
\\[1em]
Once the mean ranks are calculated, the Friedman test is performed to determine whether there are statistically significant differences among the algorithms. If the null hypothesis is rejected, the Nemenyi post-hoc test is then performed to determine which pairs of algorithms differ significantly in their performance by a critical difference. Otherwise, if the null hypothesis is not rejected, it can be concluded that there are no statistically significant differences among the algorithms on that class of problems, and each algorithm can be considered as good as the others on that class.
\\[1em]
Either way, the results obtained from this procedure allow determining which algorithms perform better and can be selected for the next step, which is the comparison between the remaining algorithms of each class.
\subsubsection{First filtering}
After the first step, a similar procedure is performed this time on the surviving algorithms of each class, and with $30$ seeds for problem, in order to obtain more robust results. The experiments are again aggregated by class and the Friedman test is performed with the Nemenyi post-hoc test, in order to determine a subset of algorithms which performances statistically differ from the others on each class, and that pass to the next step.
\subsubsection{Second filtering}
The very same procedure of the first filtering is performed again, but with $60$ seeds for $6$ problems for each class, in order to obtain even more robust results. The statistical tests are performed again, and the best algorithm is picked for the last selection step. The problems considered in this step are half new and half from the previous step, in order to test the algorithms on new problems while still maintaining some consistency with the previous step.
\\[1em]
In this case, the best algorithm is selected not only on the basis of the statistical tests, but also by considering the mean ranks obtained from the Friedman ranking. This is because statistical tests alone are not able to identify a single best algorithm without further increasing the number of experiments to an extent that cannot be determined a priori, as it depends on the observed differences in mean ranks among the algorithms.
\\[1em]
For this reason, the best algorithm is chosen as the one with the lowest mean rank, among those that are statistically significantly better than the others. It will be improperly referred as the "best algorithm" of the class, even though it is not statistically proven. The statistical tests only allow to determine the best subset of algorithms of that class (or the best algorithm if the subset contains only one algorithm).
\subsubsection{Best algorithms on each class}
The retrieval of the best algorithm for each class requires to perform a really high number of experiments, not known a priori, since it depends on the observed differences in mean ranks among the algorithms. In an extreme case, it may happen that two algorithms may be equal and thus make impossible to determine a single best algorithm. For this reason, the procedure is not based on statistical tests, but it is a free selection from the set of algorithms that are statistically significantly better than the others, usually taking into account the mean ranks obtained from the previous step.
\\[1em]
Since the problems are very different from each other, and the algorithms may perform differently on different problems, it is not possible to determine a single best algorithm for all the classes. For this reason, the best algorithm of each class is selected, thus three algorithms are selected for the final comparison.
% After the second filtering, the best algorithm of each class is selected, and then a final comparison is performed between these three algorithms on all the problems of all the classes, with $100$ seeds for each problem. This time, the mean ranks are not averaged by class, but they are calculated on the overall results obtained from all the problems and seeds, in order to obtain an estimation of the overall performance: $$\bar r_j=\dfrac1{n\cdot m\cdot c}\sum_{\substack{i=1,\ldots,n\\k=1,\ldots,m\\c=1,\ldots,C}}r_{ijkc}$$
% Where $C=3$ is the number of classes, and $n$, $m$, and $r_{ijkc}$ are defined as before. The Friedman test is performed again to determine whether there are statistically significant differences among the three algorithms, and if the null hypothesis is rejected, the Nemenyi post-hoc test is performed to determine which pairs of algorithms differ significantly in their performance by a critical difference. Otherwise, if the null hypothesis is not rejected, it can be concluded that there are no statistically significant differences among the three algorithms.
% \\[1em] In either case, the algorithm is chosen as the same way as before, which will be improperly referred as the "best algorithm" of the family.
\subsubsection{Final comparison}
In the final comparison a couple of algorithms are compared: the best algorithm of each family for that specific class. Since there are only two algorithms, the Friedman test is not applicable, and the Wilcoxon test is performed instead, in order to determine whether there are statistically significant differences between the two algorithms on that class of problems. In this case, in order to obtain more information about the performance of the two algorithms, the comparison is performed on each problem separately. Also, various statistics are calculated for each problem, such as the mean, the median and the standard deviation of the error from the optimal solution, in order to have a more complete picture of the performance of the two algorithms on each problem.
\section{Results discussion}
\subsection{PSO}
In the following subsection, the results obtained from the experiments on the PSO algorithms are discussed, along with the influence of the different parameters on the performance of the algorithms in the different classes of problems. The results are discussed separately for each step and each class.
\subsubsection{All algorithms on each class}
Each of the $96$ algorithms is tested on three problems for each class, thus obtaining $96$ mean ranks for each class. In the table \ref{tab:results_step1} the mean ranks of the algorithms are reported, along with the number of experiments that reached the threshold of $10^{-8}$ for the problems of that class. 
\begin{longtable}{||ccccc||cc||cc||cc||}
\caption{First step results for the PSO algorithms}
\label{tab:results_step1} \\
\hline
    \multicolumn{5}{||c||}{Algorithm parameters}  & \multicolumn{2}{c||}{Class 1} & \multicolumn{2}{c||}{Class 2} & \multicolumn{2}{c||}{Class 3} \\
    \hline
    \hline
    gw & inertia & lw & pop. & topology & Rank & $10^{-8}$ & Rank & $10^{-8}$ & Rank & $10^{-8}$ \\
    \hline
1.25 & 0.7 & 0.75 & 169 & Random & 13.3 & 10 & 15.8 & 0 & 12.5 & 0 \\
1.25 & 0.7 & 1.25 & 169 & Random & 12.2 & 10 & 19.1 & 0 & 14.2 & 0 \\
1.25 & 0.7 & 0.75 & 121 & Random & 16.3 & 10 & 17.5 & 0 & 12.2 & 1 \\
1.25 & 0.7 & 1.25 & 121 & Random & 16.4 & 10 & 20.8 & 0 & 9.0 & 0 \\
0.75 & 0.9 & 0.75 & 169 & Torus & 30.7 & 10 & 20.0 & 0 & 13.2 & 0 \\
0.75 & 0.7 & 1.25 & 169 & Random & 16.8 & 10 & 23.0 & 0 & 26.9 & 0 \\
0.75 & 0.9 & 0.75 & 121 & Torus & 29.0 & 10 & 16.7 & 0 & 22.7 & 0 \\
0.75 & 0.9 & 0.75 & 121 & Random & 26.1 & 10 & 39.0 & 0 & 16.4 & 0 \\
0.75 & 0.7 & 1.25 & 121 & Random & 18.1 & 10 & 29.9 & 0 & 33.8 & 0 \\
0.75 & 0.7 & 0.75 & 169 & Random & 15.9 & 10 & 30.6 & 0 & 39.6 & 0 \\
0.75 & 0.9 & 1.25 & 121 & Torus & 40.0 & 0 & 26.9 & 0 & 20.8 & 0 \\
1.25 & 0.9 & 0.75 & 121 & Torus & 56.8 & 0 & 19.4 & 0 & 17.4 & 0 \\
0.75 & 0.9 & 0.75 & 169 & Random & 35.8 & 10 & 42.6 & 0 & 16.4 & 0 \\
0.75 & 0.9 & 1.25 & 169 & Torus & 46.2 & 0 & 32.4 & 0 & 20.9 & 0 \\
1.25 & 0.9 & 0.75 & 169 & Torus & 60.8 & 0 & 22.2 & 0 & 18.5 & 0 \\
0.75 & 0.7 & 0.75 & 121 & Random & 20.6 & 10 & 36.5 & 0 & 47.8 & 0 \\
1.25 & 0.7 & 1.25 & 169 & Torus & 26.0 & 10 & 48.0 & 0 & 51.2 & 0 \\
1.25 & 0.7 & 1.25 & 121 & Torus & 23.8 & 10 & 48.5 & 0 & 55.2 & 0 \\
1.25 & 0.7 & 0.75 & 169 & Torus & 27.9 & 10 & 51.8 & 0 & 61.6 & 0 \\
0.75 & 0.9 & 1.25 & 169 & Random & 61.8 & 0 & 48.2 & 0 & 32.6 & 0 \\
1.25 & 0.7 & 0.75 & 121 & Torus & 24.0 & 10 & 57.1 & 0 & 61.7 & 0 \\
0.75 & 0.9 & 1.25 & 121 & Random & 58.8 & 0 & 52.6 & 0 & 33.4 & 0 \\
0.75 & 0.9 & 1.25 & 121 & Star & 48.6 & 1 & 53.2 & 0 & 49.1 & 0 \\
0.75 & 0.9 & 1.25 & 169 & Star & 50.6 & 0 & 50.4 & 0 & 50.2 & 0 \\
0.75 & 0.7 & 1.25 & 169 & Torus & 31.7 & 10 & 54.6 & 0 & 66.6 & 0 \\
0.75 & 0.7 & 1.25 & 121 & Torus & 30.1 & 10 & 55.4 & 0 & 70.6 & 0 \\
1.25 & 0.9 & 1.25 & 121 & Torus & 77.5 & 0 & 44.7 & 0 & 35.1 & 0 \\
1.25 & 0.9 & 0.75 & 121 & Random & 73.4 & 0 & 46.8 & 0 & 42.0 & 0 \\
0.75 & 0.7 & 0.75 & 169 & Torus & 33.3 & 10 & 58.4 & 0 & 70.8 & 0 \\
1.25 & 0.9 & 1.25 & 169 & Torus & 78.2 & 0 & 50.1 & 0 & 35.6 & 0 \\
1.25 & 0.9 & 0.75 & 169 & Random & 73.3 & 0 & 48.2 & 0 & 44.5 & 0 \\
0.75 & 0.7 & 0.75 & 121 & Torus & 30.5 & 10 & 62.2 & 0 & 78.2 & 0 \\
0.75 & 0.9 & 0.75 & 121 & Star & 38.3 & 1 & 64.5 & 0 & 71.6 & 0 \\
0.75 & 0.9 & 0.75 & 169 & Star & 37.7 & 1 & 66.4 & 0 & 71.8 & 0 \\
1.25 & 0.9 & 1.25 & 169 & Random & 79.6 & 0 & 53.4 & 0 & 46.6 & 0 \\
1.25 & 0.9 & 1.25 & 121 & Random & 82.0 & 0 & 52.8 & 0 & 46.2 & 0 \\
1.25 & 0.9 & 0.75 & 121 & Star & 64.8 & 0 & 67.6 & 0 & 55.2 & 0 \\
1.25 & 0.9 & 0.75 & 169 & Star & 68.0 & 0 & 63.8 & 0 & 57.4 & 0 \\
1.25 & 0.7 & 1.25 & 169 & Star & 56.8 & 0 & 63.7 & 0 & 83.8 & 0 \\
1.25 & 0.9 & 1.25 & 169 & Star & 80.5 & 0 & 66.0 & 0 & 58.8 & 0 \\
1.25 & 0.9 & 1.25 & 121 & Star & 81.4 & 0 & 71.4 & 0 & 57.6 & 0 \\
1.25 & 0.7 & 1.25 & 121 & Star & 60.8 & 0 & 70.7 & 0 & 82.2 & 0 \\
1.25 & 0.7 & 0.75 & 169 & Star & 64.8 & 0 & 72.2 & 0 & 85.5 & 0 \\
1.25 & 0.7 & 0.75 & 121 & Star & 70.7 & 0 & 75.2 & 0 & 86.8 & 0 \\
0.75 & 0.7 & 1.25 & 169 & Star & 80.7 & 0 & 69.7 & 0 & 84.3 & 0 \\
0.75 & 0.7 & 1.25 & 121 & Star & 85.4 & 0 & 68.8 & 0 & 82.4 & 0 \\
0.75 & 0.7 & 0.75 & 169 & Star & 85.0 & 0 & 80.0 & 0 & 87.3 & 0 \\
0.75 & 0.7 & 0.75 & 121 & Star & 87.2 & 0 & 79.2 & 0 & 89.7 & 0 \\
\hline
\end{longtable}
\noindent In order to obtain a better visualization of the results, only one table is adopted for all the classes, since the algorithms are the same, and the results are reported in different columns. The algorithms are sorted by their mean rank on the three classes, which is not a meaningful metric, but it allows to have a better visualization of the results. In the first section of the table the algorithms are reported by their parameters. The next $3$ section report mean rank and number of experiments that reached the threshold, for each class. The \texttt{end\_inertia} parameter is not reported in the table since it does not seem to have an influence on the performance of the algorithms, since almost all the algorithms with the same parameters, with and without the linear decay.
\\[1em] 
What emerges from the results is the poor performance of the Star topology, since most of the algorithms with that topology are ranked in the last positions for all the classes. On the other hand, the Random and Torus topologies are more competitive, since they are ranked in the first positions.
\\[1em]
Regarding the other parameters, the population size does not seem to have a clear influence, neither does the local weight. Meanwhile, the inertia and global weights seem to be more influential, but they need further investigation. 
\\[1em]
In order to have a statistical confirmation of these observations, the Friedman test is performed on the results of each class, and the Nemenyi post-hoc test is performed if the null hypothesis is rejected. Since the Friedman rejects the null hypothesis for all the classes with a $p$-value less than 0.025, the Nemenyi post-hoc test is performed, and the algorithm are plotted in the critical difference diagram in figure \ref{fig:cd1}.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 1}
        \includegraphics[width=0.66\textwidth]{imgs/pso/step1/1_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 2}
        \includegraphics[width=0.66\textwidth]{imgs/pso/step1/2_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 3}
        \includegraphics[width=0.66\textwidth]{imgs/pso/step1/3_critical_difference.png}
    \end{subfigure}
    \caption{Critical Difference diagrams for PSO algorithms}
    \label{fig:cd1}
\end{figure}
\noindent These diagrams plot the algorithms on a line according to their mean rank, while the other axis reports the population size. Also, the topology of the algorithms is represented by the color of the dots, while the other parameters are not shown. The critical difference is represented by the horizontal black line, which contains the algorithms that are not statistically significantly different from each other, but are statistically significantly different from the algorithms that are not contained in it.
\\[1em]
From the diagrams, it can be observed that the Star topology is statistically significantly worse, especially for the second and third classes, where only a few algorithms with that topology are not statistically significantly worse than the others. On the other hand, in the first class the Star topology performs a bit better. This is somewhat expected, since the Star topology is a simple topology that easily halts on local minima, which are not present in the first class problems. On the other hand, the Random and Torus topologies are more competitive, since almost equally present in the statistical groups of the better performing algorithms. The population has very little influence on the performance of the algorithms, while in the first figure it seems to have a good influence, in the second it has no influence, while in the third it has a negative influence. 
\\[1em]
An interesting observation is that the Random topology is far more competitive than expected, since it is a simple topology that does not have any particular structure. Unfortunately, this aspect won't be further investigated in the next steps, since as the seeds change, also the topology structure changes, thus making it impossible to determine whether the good performance of the Random topology is due to the topology itself or to the seeds used in this step. One reason behind its good performance may be that the velocity clamping is set to a small value, which may help to avoid the divergence of the particles, thus making the algorithm more stable and competitive.
\subsubsection{First filtering}
In this step, the algorithms that are not statistically significantly worse than the others are selected for each class, and then they are compared again with $30$ seeds on the same problems, in order to obtain more robust results. Since in this case the tests are done independently for each class, the results are reported in a table where, for ease of visualization, the algorithms are first sorted by the number of classes they are tested on, and then by their mean rank on the classes. If an algorithm is not ranked in a class, its mean rank for that class is not reported, since non-existing. This way, the algorithms that are excluded from some classes are easily identifiable. The results are reported in table \ref{tab:results_step2}.
\begin{longtable}{||ccccc||cc||cc||cc||}
\caption{Second step results for the PSO algorithms}
\label{tab:results_step2} \\
\hline
\multicolumn{5}{||c||}{Algorithm parameters}  & \multicolumn{2}{c||}{Class 1} & \multicolumn{2}{c||}{Class 2} & \multicolumn{2}{c||}{Class 3} \\
\hline
\hline
gw & inertia & lw & pop. & topology & Rank & $10^{-8}$ & Rank & $10^{-8}$ & Rank & $10^{-8}$ \\
\hline
1.25 & 0.7 & 0.75 & 169 & Random & 12.4 & 40 & 13.4 & 0 & 9.7 & 0 \\
1.25 & 0.7 & 1.25 & 169 & Random & 13.5 & 40 & 15.9 & 0 & 9.9 & 0 \\
1.25 & 0.7 & 0.75 & 121 & Random & 15.3 & 40 & 16.6 & 0 & 10.3 & 1 \\
1.25 & 0.7 & 1.25 & 121 & Random & 17.9 & 41 & 18.0 & 0 & 10.4 & 0 \\
0.75 & 0.7 & 1.25 & 169 & Random & 13.7 & 40 & 22.0 & 0 & 23.0 & 0 \\
0.75 & 0.9 & 0.75 & 169 & Torus & 32.0 & 40 & 17.8 & 0 & 12.7 & 0 \\
0.75 & 0.9 & 0.75 & 121 & Torus & 29.3 & 40 & 18.6 & 0 & 17.4 & 0 \\
0.75 & 0.7 & 1.25 & 121 & Random & 17.1 & 40 & 27.8 & 0 & 26.1 & 0 \\
0.75 & 0.7 & 0.75 & 169 & Random & 14.8 & 40 & 28.2 & 0 & 31.4 & 0 \\
0.75 & 0.9 & 1.25 & 121 & Torus & 38.5 & 0 & 24.0 & 0 & 14.9 & 0 \\
0.75 & 0.9 & 0.75 & 121 & Random & 28.9 & 40 & 33.6 & 0 & 15.3 & 0 \\
0.75 & 0.9 & 0.75 & 169 & Random & 32.9 & 40 & 32.8 & 0 & 14.3 & 0 \\
1.25 & 0.9 & 0.75 & 121 & Torus & 51.8 & 0 & 17.8 & 0 & 12.6 & 0 \\
0.75 & 0.7 & 0.75 & 121 & Random & 16.7 & 40 & 33.6 & 0 & 35.5 & 0 \\
0.75 & 0.9 & 1.25 & 169 & Torus & 44.6 & 0 & 27.8 & 0 & 15.3 & 0 \\
1.25 & 0.7 & 1.25 & 169 & Torus & 26.7 & 40 & 41.4 & 0 & 34.9 & 0 \\
0.75 & 0.9 & 1.25 & 169 & Star & 45.0 & 1 & 45.6 & 0 & 34.2 & 0 \\
0.75 & 0.9 & 1.25 & 121 & Star & 44.3 & 8 & 46.6 & 0 & 36.7 & 0 \\
1.25 & 0.7 & 0.75 & 169 & Random & 12.4 & 40 & 13.4 & 0 & --- & --- \\
1.25 & 0.7 & 0.75 & 121 & Random & 15.3 & 40 & 16.6 & 0 & --- & --- \\
1.25 & 0.9 & 0.75 & 169 & Torus & --- & --- & 20.7 & 0 & 14.3 & 0 \\
0.75 & 0.9 & 1.25 & 169 & Random & --- & --- & 40.2 & 0 & 23.8 & 0 \\
1.25 & 0.7 & 1.25 & 121 & Torus & 22.7 & 40 & 41.8 & 0 & --- & --- \\
1.25 & 0.9 & 1.25 & 121 & Torus & --- & --- & 39.0 & 0 & 26.7 & 0 \\
0.75 & 0.9 & 1.25 & 121 & Random & --- & --- & 41.1 & 0 & 24.9 & 0 \\
1.25 & 0.9 & 1.25 & 169 & Torus & --- & --- & 40.6 & 0 & 26.6 & 0 \\
1.25 & 0.7 & 1.25 & 169 & Torus & 26.7 & 40 & 41.4 & 0 & --- & --- \\
1.25 & 0.9 & 0.75 & 121 & Torus & 51.8 & 0 & 17.8 & 0 & --- & --- \\
1.25 & 0.7 & 0.75 & 121 & Torus & 24.2 & 40 & 46.1 & 0 & --- & --- \\
1.25 & 0.9 & 0.75 & 169 & Random & --- & --- & 39.3 & 0 & 31.1 & 0 \\
1.25 & 0.7 & 0.75 & 169 & Torus & 25.2 & 40 & 46.1 & 0 & --- & --- \\
1.25 & 0.9 & 0.75 & 121 & Random & --- & --- & 40.6 & 0 & 31.4 & 0 \\
0.75 & 0.7 & 1.25 & 121 & Torus & 28.5 & 40 & 46.7 & 0 & --- & --- \\
1.25 & 0.9 & 1.25 & 121 & Random & --- & --- & 43.0 & 0 & 32.6 & 0 \\
1.25 & 0.9 & 1.25 & 169 & Random & --- & --- & 43.1 & 0 & 32.9 & 0 \\
0.75 & 0.7 & 1.25 & 169 & Torus & 32.1 & 40 & 46.6 & 0 & --- & --- \\
0.75 & 0.7 & 0.75 & 169 & Torus & 30.5 & 40 & 49.1 & 0 & --- & --- \\
0.75 & 0.9 & 1.25 & 169 & Star & 45.0 & 1 & 45.6 & 0 & --- & --- \\
1.25 & 0.9 & 0.75 & 169 & Torus & --- & --- & 20.7 & 0 & --- & --- \\
0.75 & 0.7 & 0.75 & 121 & Torus & 29.8 & 40 & --- & --- & --- & --- \\
0.75 & 0.9 & 0.75 & 169 & Star & 31.0 & 7 & --- & --- & --- & --- \\
0.75 & 0.9 & 0.75 & 121 & Star & 32.6 & 2 & --- & --- & --- & --- \\
1.25 & 0.9 & 0.75 & 169 & Random & --- & --- & 39.3 & 0 & --- & --- \\
0.75 & 0.9 & 1.25 & 169 & Random & --- & --- & 40.2 & 0 & --- & --- \\
1.25 & 0.9 & 0.75 & 121 & Random & --- & --- & 40.6 & 0 & --- & --- \\
0.75 & 0.9 & 1.25 & 121 & Random & --- & --- & 41.1 & 0 & --- & --- \\
1.25 & 0.7 & 1.25 & 169 & Star & 46.0 & 0 & --- & --- & --- & --- \\
\hline
\end{longtable}
\noindent In the second step results table it is obvious that the algorithms with the Star topology won't make it past this step, since they are ranked in the last positions for all the classes, while the algorithms with the Random and Torus topologies are still competitive. The reasoning done previously still applies to the second step results. In order to statistically confirm these observations, the Friedman and Nemenyi tests are performed again, and the critical difference diagrams are plotted in figure \ref{fig:cd2}.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 1}
        \includegraphics[width=0.66\textwidth]{imgs/pso/step2/1_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 2}
        \includegraphics[width=0.66\textwidth]{imgs/pso/step2/2_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 3}
        \includegraphics[width=0.66\textwidth]{imgs/pso/step2/3_critical_difference.png}
    \end{subfigure}
    \caption{Critical Difference diagrams for PSO algorithms}
    \label{fig:cd2}
\end{figure}
\noindent Indeed the Star topology is statistically significantly worse than the others, especially for the second and third classes, while the Random topology seems slightly better than the Torus topology, especially in the first class. The population now plays a little role in the performance of the algorithms. 
\subsubsection{Second filtering}
The remaining algorithms are tested once more with now $60$ seeds for $6$ problems for each class, the results are reported with the same format as before in table \ref{tab:results_step3}, and the critical difference diagrams are plotted in figure \ref{fig:cd3}.
\begin{longtable}{||ccccc||cc||cc||cc||}
\caption{Third step results for the PSO algorithms}
\label{tab:results_step3} \\
\hline
\multicolumn{5}{||c||}{Algorithm parameters}  & \multicolumn{2}{c||}{Class 1} & \multicolumn{2}{c||}{Class 2} & \multicolumn{2}{c||}{Class 3} \\
\hline
\hline
gw & inertia & lw & pop. & topology & Rank & $10^{-8}$ & Rank & $10^{-8}$ & Rank & $10^{-8}$ \\
\hline
\textbf{1.25} & \textbf{0.7} & \textbf{0.75} & \textbf{169} & \textbf{Random} & \textbf{9.0} & \textbf{181} & \textbf{7.4} & \textbf{60} & \textbf{6.4} & \textbf{54} \\
1.25 & 0.7 & 0.75 & 121 & Random & \textbf{9.2} & 180 & \textbf{8.5} & 60 & \textbf{7.1} & 56 \\
1.25 & 0.7 & 1.25 & 169 & Random & \textbf{9.9} & 180 & \textbf{8.9} & 60 & \textbf{6.8} & 59 \\
1.25 & 0.7 & 1.25 & 121 & Random & \textbf{10.2} & 181 & \textbf{9.7} & 60 & \textbf{7.6} & 48 \\
1.25 & 0.7 & 0.75 & 169 & Random & \textbf{9.0} & 181 & 7.4 & 60 & --- & --- \\
1.25 & 0.7 & 0.75 & 121 & Random & \textbf{9.2} & 180 & 8.5 & 60 & --- & --- \\
0.75 & 0.9 & 0.75 & 169 & Torus & --- & --- & 12.3 & 0 & 9.8 & 48 \\
0.75 & 0.7 & 1.25 & 169 & Random & \textbf{8.8} & 180 & 13.6 & 60 & --- & --- \\
0.75 & 0.9 & 0.75 & 121 & Torus & --- & --- & 12.7 & 0 & 10.5 & 59 \\
0.75 & 0.7 & 0.75 & 169 & Random & \textbf{9.4} & 180 & 17.5 & 42 & --- & --- \\
0.75 & 0.7 & 1.25 & 121 & Random & \textbf{10.0} & 180 & 16.9 & 24 & --- & --- \\
1.25 & 0.9 & 0.75 & 121 & Torus & --- & --- & 15.8 & 0 & 12.0 & 0 \\
1.25 & 0.9 & 0.75 & 169 & Torus & --- & --- & 16.3 & 0 & 12.8 & 0 \\
0.75 & 0.9 & 1.25 & 121 & Torus & --- & --- & 17.5 & 0 & 13.5 & 0 \\
0.75 & 0.9 & 1.25 & 169 & Torus & --- & --- & 18.3 & 0 & 13.2 & 0 \\
0.75 & 0.7 & 0.75 & 121 & Random & \textbf{10.7} & 180 & --- & --- & --- & --- \\
0.75 & 0.9 & 0.75 & 169 & Random & --- & --- & --- & --- & 11.8 & 0 \\
0.75 & 0.9 & 0.75 & 121 & Random & --- & --- & --- & --- & 12.7 & 10 \\
1.25 & 0.7 & 0.75 & 121 & Torus & 13.7 & 180 & --- & --- & --- & --- \\
1.25 & 0.7 & 1.25 & 121 & Torus & 14.1 & 180 & --- & --- & --- & --- \\
1.25 & 0.9 & 0.75 & 121 & Torus & --- & --- & 15.8 & 0 & --- & --- \\
1.25 & 0.9 & 0.75 & 169 & Torus & --- & --- & 16.3 & 0 & --- & --- \\\hline
\end{longtable}
\noindent One particular observation is that now the algorithms hit more thresholds than before, it is now seldom to have algorithms that do not reach the threshold at all. This may be due to the fact that the algorithms are tested on easier problems.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 1}
        \includegraphics[width=0.66\textwidth]{imgs/pso/step3/1_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 2}
        \includegraphics[width=0.66\textwidth]{imgs/pso/step3/2_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 3}
        \includegraphics[width=0.66\textwidth]{imgs/pso/step3/3_critical_difference.png}
    \end{subfigure}
    \caption{Critical Difference diagrams for PSO algorithms}
    \label{fig:cd3}
\end{figure}
\noindent From the critical difference diagrams, it can be observed that the algorithms with the Random topology made it to the best performing group for all the classes, while the algorithms with the Torus topology were left behind. Also in this case the linear decay of the inertia does not seem to have an influence on the performance of the algorithms. 
\\[1em]
The first algorithm shown in the table is also the first algorithm of each class, since it has the highest rank for all the classes, thus making it the "best algorithm" of the family. Albeit, it is important to note that it is not statistically significantly better than the other algorithms in the same group, making it not possible to declare it as the best algorithm of the family.
\\[1em]
The fact that the same algorithm is the best for all the classes means it is robust to the different types of problems. Since the next step supposes to compare the "best" of each class, it will be skipped.
\subsection{ABC}
In the following subsection, the results obtained from the experiments on the ABC algorithms are discussed, along with the influence of the different parameters on the performance of the algorithms in the different classes of problems. The results are discussed separately for each step and each class.
\subsubsection{All algorithms on each class}
Each of the $108$ algorithms is tested on three problems for each class, thus obtaining $108$ mean ranks for each class. In the following tables the mean ranks of the algorithms are reported, for each class. The inizialization strategy is not reported in tables because its the same for all the algorithms. 
\begin{longtable}{||cccc||cc||cc||cc||}
\caption{Risultati aggregati}
\label{tab:results} \\
\hline
    \multicolumn{4}{||c||}{Algorithm parameters}  & \multicolumn{2}{c||}{Class 1} & \multicolumn{2}{c||}{Class 2} & \multicolumn{2}{c||}{Class 3} \\
    \hline
    \hline
    \texttt{limit} & \texttt{max\_scouts} & \texttt{mut} &  \texttt{tour\_size} & Rank & $10^{-8}$ & Rank & $10^{-8}$ & Rank & $10^{-8}$ \\
    \hline
2000 & 15 & ModifiedABC & 7 & 25.40 & 0 & 45.50 & 0 & 9.80 & 0 \\
3000 & 10 & ModifiedABC & 7 & 25.40 & 0 & 45.50 & 0 & 9.80 & 0 \\
2000 & 20 & ModifiedABC & 7 & 25.40 & 0 & 45.50 & 0 & 9.80 & 0 \\
3000 & 15 & ModifiedABC & 7 & 25.40 & 0 & 45.50 & 0 & 9.80 & 0 \\
2500 & 20 & ModifiedABC & 7 & 25.40 & 0 & 45.50 & 0 & 9.80 & 0 \\
2000 & 10 & ModifiedABC & 7 & 25.40 & 0 & 45.50 & 0 & 9.80 & 0 \\
3000 & 20 & ModifiedABC & 7 & 25.40 & 0 & 45.50 & 0 & 9.80 & 0 \\
2500 & 10 & ModifiedABC & 7 & 25.40 & 0 & 45.50 & 0 & 9.80 & 0 \\
2500 & 15 & ModifiedABC & 7 & 25.40 & 0 & 45.50 & 0 & 9.80 & 0 \\
2500 & 20 & ModifiedABC & 5 & 43.10 & 0 & 55.70 & 0 & 12.20 & 0 \\
2500 & 10 & ModifiedABC & 5 & 43.10 & 0 & 55.70 & 0 & 12.20 & 0 \\
2000 & 10 & ModifiedABC & 5 & 43.10 & 0 & 55.70 & 0 & 12.20 & 0 \\
3000 & 15 & ModifiedABC & 5 & 43.10 & 0 & 55.70 & 0 & 12.20 & 0 \\
2000 & 15 & ModifiedABC & 5 & 43.10 & 0 & 55.70 & 0 & 12.20 & 0 \\
2000 & 20 & ModifiedABC & 5 & 43.10 & 0 & 55.70 & 0 & 12.20 & 0 \\
3000 & 10 & ModifiedABC & 5 & 43.10 & 0 & 55.70 & 0 & 12.20 & 0 \\
3000 & 20 & ModifiedABC & 5 & 43.10 & 0 & 55.70 & 0 & 12.20 & 0 \\
2500 & 15 & ModifiedABC & 5 & 43.10 & 0 & 55.70 & 0 & 12.20 & 0 \\
2000 & 10 & DirectedABC & 5 & 20.00 & 0 & 33.50 & 0 & 60.50 & 0 \\
2000 & 15 & DirectedABC & 5 & 20.00 & 0 & 33.50 & 0 & 60.50 & 0 \\
2000 & 20 & DirectedABC & 5 & 20.00 & 0 & 33.50 & 0 & 60.50 & 0 \\
3000 & 15 & DirectedABC & 5 & 20.00 & 0 & 33.50 & 0 & 60.50 & 0 \\
2500 & 15 & DirectedABC & 5 & 20.00 & 0 & 33.50 & 0 & 60.50 & 0 \\
2500 & 10 & DirectedABC & 5 & 20.00 & 0 & 33.50 & 0 & 60.50 & 0 \\
2500 & 20 & DirectedABC & 5 & 20.00 & 0 & 33.50 & 0 & 60.50 & 0 \\
3000 & 10 & DirectedABC & 5 & 20.00 & 0 & 33.50 & 0 & 60.50 & 0 \\
3000 & 20 & DirectedABC & 5 & 20.00 & 0 & 33.50 & 0 & 60.50 & 0 \\
2000 & 10 & DirectedABC & 7 & 23.60 & 0 & 35.00 & 0 & 59.90 & 0 \\
3000 & 10 & DirectedABC & 7 & 23.60 & 0 & 35.00 & 0 & 59.90 & 0 \\
2000 & 15 & DirectedABC & 7 & 23.60 & 0 & 35.00 & 0 & 59.90 & 0 \\
2500 & 20 & DirectedABC & 7 & 23.60 & 0 & 35.00 & 0 & 59.90 & 0 \\
3000 & 15 & DirectedABC & 7 & 23.60 & 0 & 35.00 & 0 & 59.90 & 0 \\
2000 & 20 & DirectedABC & 7 & 23.60 & 0 & 35.00 & 0 & 59.90 & 0 \\
3000 & 20 & DirectedABC & 7 & 23.60 & 0 & 35.00 & 0 & 59.90 & 0 \\
2500 & 10 & DirectedABC & 7 & 23.60 & 0 & 35.00 & 0 & 59.90 & 0 \\
2500 & 15 & DirectedABC & 7 & 23.60 & 0 & 35.00 & 0 & 59.90 & 0 \\
2000 & 10 & DirectedABC & 3 & 37.40 & 0 & 46.10 & 0 & 63.20 & 0 \\
3000 & 20 & DirectedABC & 3 & 37.40 & 0 & 46.10 & 0 & 63.20 & 0 \\
2000 & 20 & DirectedABC & 3 & 37.40 & 0 & 46.10 & 0 & 63.20 & 0 \\
3000 & 15 & DirectedABC & 3 & 37.40 & 0 & 46.10 & 0 & 63.20 & 0 \\
2500 & 10 & DirectedABC & 3 & 37.40 & 0 & 46.10 & 0 & 63.20 & 0 \\
2500 & 20 & DirectedABC & 3 & 37.40 & 0 & 46.10 & 0 & 63.20 & 0 \\
3000 & 10 & DirectedABC & 3 & 37.40 & 0 & 46.10 & 0 & 63.20 & 0 \\
2000 & 15 & DirectedABC & 3 & 37.40 & 0 & 46.10 & 0 & 63.20 & 0 \\
2500 & 15 & DirectedABC & 3 & 37.40 & 0 & 46.10 & 0 & 63.20 & 0 \\
3000 & 20 & StandardABC & 7 & 46.70 & 0 & 40.40 & 0 & 65.30 & 0 \\
2000 & 10 & StandardABC & 7 & 46.70 & 0 & 40.40 & 0 & 65.30 & 0 \\
2500 & 15 & StandardABC & 7 & 46.70 & 0 & 40.40 & 0 & 65.30 & 0 \\
2500 & 10 & StandardABC & 7 & 46.70 & 0 & 40.40 & 0 & 65.30 & 0 \\
2500 & 20 & StandardABC & 7 & 46.70 & 0 & 40.40 & 0 & 65.30 & 0 \\
3000 & 15 & StandardABC & 7 & 46.70 & 0 & 40.40 & 0 & 65.30 & 0 \\
2000 & 20 & StandardABC & 7 & 46.70 & 0 & 40.40 & 0 & 65.30 & 0 \\
3000 & 10 & StandardABC & 7 & 46.70 & 0 & 40.40 & 0 & 65.30 & 0 \\
2000 & 15 & StandardABC & 7 & 46.70 & 0 & 40.40 & 0 & 65.30 & 0 \\
2500 & 20 & StandardABC & 5 & 45.50 & 0 & 42.20 & 0 & 65.30 & 0 \\
3000 & 15 & StandardABC & 5 & 45.50 & 0 & 42.20 & 0 & 65.30 & 0 \\
2000 & 15 & StandardABC & 5 & 45.50 & 0 & 42.20 & 0 & 65.30 & 0 \\
2500 & 10 & StandardABC & 5 & 45.50 & 0 & 42.20 & 0 & 65.30 & 0 \\
2500 & 15 & StandardABC & 5 & 45.50 & 0 & 42.20 & 0 & 65.30 & 0 \\
2000 & 10 & StandardABC & 5 & 45.50 & 0 & 42.20 & 0 & 65.30 & 0 \\
3000 & 20 & StandardABC & 5 & 45.50 & 0 & 42.20 & 0 & 65.30 & 0 \\
3000 & 10 & StandardABC & 5 & 45.50 & 0 & 42.20 & 0 & 65.30 & 0 \\
2000 & 20 & StandardABC & 5 & 45.50 & 0 & 42.20 & 0 & 65.30 & 0 \\
2500 & 10 & ModifiedABC & 3 & 70.70 & 0 & 62.60 & 0 & 20.00 & 0 \\
3000 & 10 & ModifiedABC & 3 & 70.70 & 0 & 62.60 & 0 & 20.00 & 0 \\
2000 & 15 & ModifiedABC & 3 & 70.70 & 0 & 62.60 & 0 & 20.00 & 0 \\
2000 & 10 & ModifiedABC & 3 & 70.70 & 0 & 62.60 & 0 & 20.00 & 0 \\
2000 & 20 & ModifiedABC & 3 & 70.70 & 0 & 62.60 & 0 & 20.00 & 0 \\
3000 & 15 & ModifiedABC & 3 & 70.70 & 0 & 62.60 & 0 & 20.00 & 0 \\
3000 & 20 & ModifiedABC & 3 & 70.70 & 0 & 62.60 & 0 & 20.00 & 0 \\
2500 & 15 & ModifiedABC & 3 & 70.70 & 0 & 62.60 & 0 & 20.00 & 0 \\
2500 & 20 & ModifiedABC & 3 & 70.70 & 0 & 62.60 & 0 & 20.00 & 0 \\
2500 & 20 & StandardABC & 3 & 56.60 & 0 & 49.70 & 0 & 71.90 & 0 \\
2500 & 10 & StandardABC & 3 & 56.60 & 0 & 49.70 & 0 & 71.90 & 0 \\
2000 & 10 & StandardABC & 3 & 56.60 & 0 & 49.70 & 0 & 71.90 & 0 \\
3000 & 20 & StandardABC & 3 & 56.60 & 0 & 49.70 & 0 & 71.90 & 0 \\
2500 & 15 & StandardABC & 3 & 56.60 & 0 & 49.70 & 0 & 71.90 & 0 \\
2000 & 15 & StandardABC & 3 & 56.60 & 0 & 49.70 & 0 & 71.90 & 0 \\
3000 & 15 & StandardABC & 3 & 56.60 & 0 & 49.70 & 0 & 71.90 & 0 \\
2000 & 20 & StandardABC & 3 & 56.60 & 0 & 49.70 & 0 & 71.90 & 0 \\
3000 & 10 & StandardABC & 3 & 56.60 & 0 & 49.70 & 0 & 71.90 & 0 \\
3000 & 10 & iABC & 5 & 91.40 & 0 & 75.20 & 0 & 74.30 & 0 \\
2000 & 15 & iABC & 5 & 91.40 & 0 & 75.20 & 0 & 74.30 & 0 \\
2000 & 20 & iABC & 5 & 91.40 & 0 & 75.20 & 0 & 74.30 & 0 \\
3000 & 15 & iABC & 5 & 91.40 & 0 & 75.20 & 0 & 74.30 & 0 \\
2500 & 10 & iABC & 5 & 91.40 & 0 & 75.20 & 0 & 74.30 & 0 \\
2000 & 10 & iABC & 5 & 91.40 & 0 & 75.20 & 0 & 74.30 & 0 \\
2500 & 15 & iABC & 5 & 91.40 & 0 & 75.20 & 0 & 74.30 & 0 \\
3000 & 20 & iABC & 5 & 91.40 & 0 & 75.20 & 0 & 74.30 & 0 \\
2500 & 20 & iABC & 5 & 91.40 & 0 & 75.20 & 0 & 74.30 & 0 \\
3000 & 20 & iABC & 7 & 94.70 & 0 & 80.00 & 0 & 81.80 & 0 \\
2000 & 20 & iABC & 7 & 94.70 & 0 & 80.00 & 0 & 81.80 & 0 \\
2500 & 15 & iABC & 7 & 94.70 & 0 & 80.00 & 0 & 81.80 & 0 \\
2000 & 10 & iABC & 7 & 94.70 & 0 & 80.00 & 0 & 81.80 & 0 \\
2000 & 15 & iABC & 7 & 94.70 & 0 & 80.00 & 0 & 81.80 & 0 \\
3000 & 15 & iABC & 7 & 94.70 & 0 & 80.00 & 0 & 81.80 & 0 \\
3000 & 10 & iABC & 7 & 94.70 & 0 & 80.00 & 0 & 81.80 & 0 \\
2500 & 20 & iABC & 7 & 94.70 & 0 & 80.00 & 0 & 81.80 & 0 \\
2500 & 10 & iABC & 7 & 94.70 & 0 & 80.00 & 0 & 81.80 & 0 \\
3000 & 20 & iABC & 3 & 98.90 & 0 & 88.10 & 0 & 69.80 & 0 \\
2000 & 10 & iABC & 3 & 98.90 & 0 & 88.10 & 0 & 69.80 & 0 \\
2500 & 20 & iABC & 3 & 98.90 & 0 & 88.10 & 0 & 69.80 & 0 \\
2500 & 10 & iABC & 3 & 98.90 & 0 & 88.10 & 0 & 69.80 & 0 \\
2500 & 15 & iABC & 3 & 98.90 & 0 & 88.10 & 0 & 69.80 & 0 \\
3000 & 10 & iABC & 3 & 98.90 & 0 & 88.10 & 0 & 69.80 & 0 \\
2000 & 20 & iABC & 3 & 98.90 & 0 & 88.10 & 0 & 69.80 & 0 \\
2000 & 15 & iABC & 3 & 98.90 & 0 & 88.10 & 0 & 69.80 & 0 \\
3000 & 15 & iABC & 3 & 98.90 & 0 & 88.10 & 0 & 69.80 & 0 \\
\hline
\end{longtable}
From the results we can see the poor performance of algorithms with mutation strategy "iABC", which are ranked in the last positions for all the classes, while the algorithms with mutation strategy "ModifiedABC" are ranked in the first positions for all the classes. The algorithms with mutation strategy "StandardABC" are ranked in the middle positions for all the classes. The influence of the other parameters is not clear from the results, since there are algorithms with different parameters that are ranked in similar positions. In order to statistically confirm these observations, the Friedman and Nemenyi tests are performed, and the critical difference diagrams are plotted in figure \ref{fig:cd1}.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 1}
        \includegraphics[width=0.66\textwidth]{imgs/abc/step1/1_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 2}
        \includegraphics[width=0.66\textwidth]{imgs/abc/step1/2_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 3}
        \includegraphics[width=0.66\textwidth]{imgs/abc/step1/3_critical_difference.png}
    \end{subfigure}
    \caption{Critical Difference diagrams for ABC algorithms}
    \label{fig:cd1}
\end{figure}
\noindent
In these scatter plots, each point represents an algorithm, and the x-axis represents the rank of the algorithm. The y-axis represents the tournament size parameter. The color of the points represents the mutation operator used in the algorithm. 
From the critical difference diagrams, it can be observed that the algorithms with mutation strategy "ModifiedABC" are statistically significantly better than the algorithms with mutation strategy "iABC", while the algorithms with mutation strategy "StandardABC" are not statistically significantly different from the other two groups. The influence of the tournament size parameter is not clear from the diagrams, since there are algorithms with different tournament sizes that are ranked in similar positions. 

\subsection{First filtering}
The first filtering is done by testing all the algorithms on three problems for each class, with $30$ seeds. The results are reported in table \ref{tab:results_step2}, and the critical difference diagrams are plotted in figure \ref{fig:cd2}.

\begin{longtable}{||cccc||cc||cc||cc||}
\caption{Second step results for the ABC algorithms}
\label{tab:results_step2} \\
\hline
\multicolumn{4}{||c||}{Algorithm parameters}  & \multicolumn{2}{c||}{Class 1} & \multicolumn{2}{c||}{Class 2} & \multicolumn{2}{c||}{Class 3} \\
\hline
\texttt{limit} & \texttt{max\_scouts} & \texttt{mut} &  \texttt{tour\_size} & Rank & $10^{-8}$ & Rank & $10^{-8}$ & Rank & $10^{-8}$ \\
\hline
2000 & 10 & ModifiedABC & 7 & 24.0 & 0 & 49.4 & 0 & 10.0 & 0 \\
2000 & 15 & ModifiedABC & 7 & 24.0 & 0 & 49.4 & 0 & 10.0 & 0 \\
2000 & 20 & ModifiedABC & 7 & 24.0 & 0 & 49.4 & 0 & 10.0 & 0 \\
2500 & 10 & ModifiedABC & 7 & 24.0 & 0 & 49.4 & 0 & 10.0 & 0 \\
2500 & 15 & ModifiedABC & 7 & 24.0 & 0 & 49.4 & 0 & 10.0 & 0 \\
2500 & 20 & ModifiedABC & 7 & 24.0 & 0 & 49.4 & 0 & 10.0 & 0 \\
3000 & 10 & ModifiedABC & 7 & 24.0 & 0 & 49.4 & 0 & 10.0 & 0 \\
3000 & 15 & ModifiedABC & 7 & 24.0 & 0 & 49.4 & 0 & 10.0 & 0 \\
3000 & 20 & ModifiedABC & 7 & 24.0 & 0 & 49.4 & 0 & 10.0 & 0 \\
2000 & 10 & DirectedABC & 7 & 24.6 & 0 & 32.4 & 0 & 36.6 & 0 \\
2000 & 15 & DirectedABC & 7 & 24.6 & 0 & 32.4 & 0 & 36.6 & 0 \\
2000 & 20 & DirectedABC & 7 & 24.6 & 0 & 32.4 & 0 & 36.6 & 0 \\
2500 & 10 & DirectedABC & 7 & 24.6 & 0 & 32.4 & 0 & 36.6 & 0 \\
2500 & 15 & DirectedABC & 7 & 24.6 & 0 & 32.4 & 0 & 36.6 & 0 \\
2500 & 20 & DirectedABC & 7 & 24.6 & 0 & 32.4 & 0 & 36.6 & 0 \\
3000 & 10 & DirectedABC & 7 & 24.6 & 0 & 32.4 & 0 & 36.6 & 0 \\
3000 & 15 & DirectedABC & 7 & 24.6 & 0 & 32.4 & 0 & 36.6 & 0 \\
3000 & 20 & DirectedABC & 7 & 24.6 & 0 & 32.4 & 0 & 36.6 & 0 \\
2000 & 10 & DirectedABC & 5 & 24.5 & 0 & 37.8 & 0 & 36.4 & 0 \\
2000 & 15 & DirectedABC & 5 & 24.5 & 0 & 37.8 & 0 & 36.4 & 0 \\
2000 & 20 & DirectedABC & 5 & 24.5 & 0 & 37.8 & 0 & 36.4 & 0 \\
2500 & 10 & DirectedABC & 5 & 24.5 & 0 & 37.8 & 0 & 36.4 & 0 \\
2500 & 15 & DirectedABC & 5 & 24.5 & 0 & 37.8 & 0 & 36.4 & 0 \\
2500 & 20 & DirectedABC & 5 & 24.5 & 0 & 37.8 & 0 & 36.4 & 0 \\
3000 & 10 & DirectedABC & 5 & 24.5 & 0 & 37.8 & 0 & 36.4 & 0 \\
3000 & 15 & DirectedABC & 5 & 24.5 & 0 & 37.8 & 0 & 36.4 & 0 \\
3000 & 20 & DirectedABC & 5 & 24.5 & 0 & 37.8 & 0 & 36.4 & 0 \\
2000 & 10 & ModifiedABC & 5 & 40.5 & 0 & 52.7 & 0 & 12.6 & 0 \\
2000 & 15 & ModifiedABC & 5 & 40.5 & 0 & 52.7 & 0 & 12.6 & 0 \\
2000 & 20 & ModifiedABC & 5 & 40.5 & 0 & 52.7 & 0 & 12.6 & 0 \\
2500 & 10 & ModifiedABC & 5 & 40.5 & 0 & 52.7 & 0 & 12.6 & 0 \\
2500 & 15 & ModifiedABC & 5 & 40.5 & 0 & 52.7 & 0 & 12.6 & 0 \\
2500 & 20 & ModifiedABC & 5 & 40.5 & 0 & 52.7 & 0 & 12.6 & 0 \\
3000 & 10 & ModifiedABC & 5 & 40.5 & 0 & 52.7 & 0 & 12.6 & 0 \\
3000 & 15 & ModifiedABC & 5 & 40.5 & 0 & 52.7 & 0 & 12.6 & 0 \\
3000 & 20 & ModifiedABC & 5 & 40.5 & 0 & 52.7 & 0 & 12.6 & 0 \\
2000 & 10 & ModifiedABC & 3 & 69.4 & 0 & 59.2 & 0 & 19.5 & 0 \\
2000 & 15 & ModifiedABC & 3 & 69.4 & 0 & 59.2 & 0 & 19.5 & 0 \\
2000 & 20 & ModifiedABC & 3 & 69.4 & 0 & 59.2 & 0 & 19.5 & 0 \\
2500 & 10 & ModifiedABC & 3 & 69.4 & 0 & 59.2 & 0 & 19.5 & 0 \\
2500 & 15 & ModifiedABC & 3 & 69.4 & 0 & 59.2 & 0 & 19.5 & 0 \\
2500 & 20 & ModifiedABC & 3 & 69.4 & 0 & 59.2 & 0 & 19.5 & 0 \\
3000 & 10 & ModifiedABC & 3 & 69.4 & 0 & 59.2 & 0 & 19.5 & 0 \\
3000 & 15 & ModifiedABC & 3 & 69.4 & 0 & 59.2 & 0 & 19.5 & 0 \\
3000 & 20 & ModifiedABC & 3 & 69.4 & 0 & 59.2 & 0 & 19.5 & 0 \\
2000 & 10 & DirectedABC & 3 & 36.8 & 0 & 44.8 & 0 & --- & --- \\
2000 & 15 & DirectedABC & 3 & 36.8 & 0 & 44.8 & 0 & --- & --- \\
2000 & 20 & DirectedABC & 3 & 36.8 & 0 & 44.8 & 0 & --- & --- \\
2500 & 10 & DirectedABC & 3 & 36.8 & 0 & 44.8 & 0 & --- & --- \\
2500 & 15 & DirectedABC & 3 & 36.8 & 0 & 44.8 & 0 & --- & --- \\
2500 & 20 & DirectedABC & 3 & 36.8 & 0 & 44.8 & 0 & --- & --- \\
3000 & 10 & DirectedABC & 3 & 36.8 & 0 & 44.8 & 0 & --- & --- \\
3000 & 15 & DirectedABC & 3 & 36.8 & 0 & 44.8 & 0 & --- & --- \\
3000 & 20 & DirectedABC & 3 & 36.8 & 0 & 44.8 & 0 & --- & --- \\
2000 & 10 & StandardABC & 7 & 45.7 & 0 & 37.0 & 0 & --- & --- \\
2000 & 15 & StandardABC & 7 & 45.7 & 0 & 37.0 & 0 & --- & --- \\
2000 & 20 & StandardABC & 7 & 45.7 & 0 & 37.0 & 0 & --- & --- \\
2500 & 10 & StandardABC & 7 & 45.7 & 0 & 37.0 & 0 & --- & --- \\
2500 & 15 & StandardABC & 7 & 45.7 & 0 & 37.0 & 0 & --- & --- \\
2500 & 20 & StandardABC & 7 & 45.7 & 0 & 37.0 & 0 & --- & --- \\
3000 & 10 & StandardABC & 7 & 45.7 & 0 & 37.0 & 0 & --- & --- \\
3000 & 15 & StandardABC & 7 & 45.7 & 0 & 37.0 & 0 & --- & --- \\
3000 & 20 & StandardABC & 7 & 45.7 & 0 & 37.0 & 0 & --- & --- \\
2000 & 10 & StandardABC & 5 & 45.1 & 0 & 39.9 & 0 & --- & --- \\
2000 & 15 & StandardABC & 5 & 45.1 & 0 & 39.9 & 0 & --- & --- \\
2000 & 20 & StandardABC & 5 & 45.1 & 0 & 39.9 & 0 & --- & --- \\
2500 & 10 & StandardABC & 5 & 45.1 & 0 & 39.9 & 0 & --- & --- \\
2500 & 15 & StandardABC & 5 & 45.1 & 0 & 39.9 & 0 & --- & --- \\
2500 & 20 & StandardABC & 5 & 45.1 & 0 & 39.9 & 0 & --- & --- \\
3000 & 10 & StandardABC & 5 & 45.1 & 0 & 39.9 & 0 & --- & --- \\
3000 & 15 & StandardABC & 5 & 45.1 & 0 & 39.9 & 0 & --- & --- \\
3000 & 20 & StandardABC & 5 & 45.1 & 0 & 39.9 & 0 & --- & --- \\
2000 & 10 & StandardABC & 3 & 58.4 & 0 & 47.8 & 0 & --- & --- \\
2000 & 15 & StandardABC & 3 & 58.4 & 0 & 47.8 & 0 & --- & --- \\
2000 & 20 & StandardABC & 3 & 58.4 & 0 & 47.8 & 0 & --- & --- \\
2500 & 10 & StandardABC & 3 & 58.4 & 0 & 47.8 & 0 & --- & --- \\
2500 & 15 & StandardABC & 3 & 58.4 & 0 & 47.8 & 0 & --- & --- \\
2500 & 20 & StandardABC & 3 & 58.4 & 0 & 47.8 & 0 & --- & --- \\
3000 & 10 & StandardABC & 3 & 58.4 & 0 & 47.8 & 0 & --- & --- \\
3000 & 15 & StandardABC & 3 & 58.4 & 0 & 47.8 & 0 & --- & --- \\
3000 & 20 & StandardABC & 3 & 58.4 & 0 & 47.8 & 0 & --- & --- \\
2000 & 10 & iABC & 5 & --- & --- & 72.8 & 0 & --- & --- \\
2000 & 15 & iABC & 5 & --- & --- & 72.8 & 0 & --- & --- \\
2000 & 20 & iABC & 5 & --- & --- & 72.8 & 0 & --- & --- \\
2500 & 10 & iABC & 5 & --- & --- & 72.8 & 0 & --- & --- \\
2500 & 15 & iABC & 5 & --- & --- & 72.8 & 0 & --- & --- \\
2500 & 20 & iABC & 5 & --- & --- & 72.8 & 0 & --- & --- \\
3000 & 10 & iABC & 5 & --- & --- & 72.8 & 0 & --- & --- \\
3000 & 15 & iABC & 5 & --- & --- & 72.8 & 0 & --- & --- \\
3000 & 20 & iABC & 5 & --- & --- & 72.8 & 0 & --- & --- \\
2000 & 10 & iABC & 7 & --- & --- & 76.3 & 0 & --- & --- \\
2000 & 15 & iABC & 7 & --- & --- & 76.3 & 0 & --- & --- \\
2000 & 20 & iABC & 7 & --- & --- & 76.3 & 0 & --- & --- \\
2500 & 10 & iABC & 7 & --- & --- & 76.3 & 0 & --- & --- \\
2500 & 15 & iABC & 7 & --- & --- & 76.3 & 0 & --- & --- \\
2500 & 20 & iABC & 7 & --- & --- & 76.3 & 0 & --- & --- \\
3000 & 10 & iABC & 7 & --- & --- & 76.3 & 0 & --- & --- \\
3000 & 15 & iABC & 7 & --- & --- & 76.3 & 0 & --- & --- \\
3000 & 20 & iABC & 7 & --- & --- & 76.3 & 0 & --- & --- \\
\hline
\end{longtable}
\noindent \\
Now we can see that the algorithms with mutation strategy "ModifiedABC" are ranked in the first positions for all the classes, while the algorithms with mutation strategy "DirectedABC" are ranked in the middle positions for all the classes, apart from the first class where they are ranked in the first positions. From previous analysis we expect that "DirectedABC" and "ModifiedABC" could be statistically significantly better than other mutation strategy, because they are the only mutation strategies that add "information" about dimensionality to the search process. As explained in the previous section,the Friedman and Nemenyi tests are performed, and the critical difference diagrams are plotted in figure \ref{fig:cd2}. 
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 1}
        \includegraphics[width=0.66\textwidth]{imgs/abc/step2/1_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 2}
        \includegraphics[width=0.66\textwidth]{imgs/abc/step2/2_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 3}
        \includegraphics[width=0.66\textwidth]{imgs/abc/step2/3_critical_difference.png}
    \end{subfigure}
    \caption{Critical Difference diagrams for ABC algorithms after the first filtering step}
    \label{fig:cd2}
\end{figure}
From the critical difference diagrams, it can be observed that the algorithms with mutation strategy "ModifiedABC" are statistically significantly better than the algorithms with mutation strategy "StandardABC" and "iABC", while the algorithms with mutation strategy "DirectedABC" are not statistically significantly different from the other two groups. The influence of the tournament size parameter is not clear from the diagrams, since there are algorithms with different tournament sizes that are ranked in similar positions. We can expect that for second and third class, the algorithms with higher limit, max\_scouts and tourrnament\_size parameters could be ranked in better positions, because they are more "explorative" and the second and third class problems are more difficult than the first class problems. However, from the critical difference diagrams we can see that there are algorithms with different parameters that are ranked in similar positions, so we cannot confirm this observation.


\subsection{Second filtering}
The second filtering is done by testing the best algorithms of the first filtering on 6 problems for each class, with $60$ seeds. The results are reported in table \ref{tab:results_step3}, and the critical difference diagrams are plotted in figure \ref{fig:cd3}.
\begin{longtable}{||cccc||cc||cc||cc||}
\caption{Third step results for the ABC algorithms}
\label{tab:results_step3} \\
\hline
\multicolumn{4}{||c||}{Algorithm parameters}  & \multicolumn{2}{c||}{Class 1} & \multicolumn{2}{c||}{Class 2} & \multicolumn{2}{c||}{Class 3} \\
\hline  
\texttt{limit} & \texttt{max\_scouts} & \texttt{mut} &  \texttt{tour\_size} & Rank & $10^{-8}$ & Rank & $10^{-8}$ & Rank & $10^{-8}$ \\
\hline

3000 & 20 & ModifiedABC & 7 & 23.8 & 0 & 6.8 & 0 & 11.2 & 0 \\
2000 & 15 & ModifiedABC & 7 & 23.8 & 0 & 6.8 & 0 & 12.4 & 0 \\
2000 & 20 & ModifiedABC & 7 & 23.8 & 0 & 18.9 & 0 & 8.7 & 0 \\
2500 & 10 & ModifiedABC & 7 & 23.8 & 0 & 18.9 & 0 & 8.7 & 0 \\
2500 & 15 & ModifiedABC & 7 & 23.8 & 0 & 18.9 & 0 & 8.7 & 0 \\
2500 & 20 & ModifiedABC & 7 & 23.8 & 0 & 18.9 & 0 & 8.7 & 0 \\
3000 & 10 & ModifiedABC & 7 & 23.8 & 0 & 18.9 & 0 & 8.7 & 0 \\
3000 & 15 & ModifiedABC & 7 & 23.8 & 0 & 18.9 & 0 & 8.7 & 0 \\
2000 & 10 & ModifiedABC & 7 & 23.8 & 0 & 6.8 & 0 & 24.2 & 0 \\
2000 & 20 & ModifiedABC & 5 & 33.1 & 0 & 21.8 & 0 & 10.3 & 0 \\
2000 & 10 & ModifiedABC & 5 & 33.1 & 0 & 21.8 & 0 & 10.4 & 0 \\
2000 & 15 & ModifiedABC & 5 & 33.1 & 0 & 21.8 & 0 & 10.4 & 0 \\
2500 & 10 & ModifiedABC & 5 & 33.1 & 0 & 21.8 & 0 & 10.4 & 0 \\
2500 & 20 & ModifiedABC & 5 & 33.1 & 0 & 21.8 & 0 & 10.4 & 0 \\
3000 & 10 & ModifiedABC & 5 & 33.1 & 0 & 21.8 & 0 & 10.4 & 0 \\
3000 & 20 & ModifiedABC & 5 & 33.1 & 0 & 21.8 & 0 & 10.4 & 0 \\
2500 & 15 & ModifiedABC & 5 & 33.1 & 0 & 21.8 & 0 & 24.4 & 0 \\
3000 & 15 & ModifiedABC & 5 & 33.1 & 0 & 21.8 & 0 & 24.4 & 0 \\
2500 & 10 & DirectedABC & 7 & 16.8 & \textbf{60} & 28.5 & 0 & --- & --- \\
2500 & 15 & DirectedABC & 7 & 16.8 & \textbf{60} & 28.5 & 0 & --- & --- \\
2500 & 20 & DirectedABC & 7 & 16.8 & \textbf{60} & 28.5 & 0 & --- & --- \\
3000 & 10 & DirectedABC & 7 & 16.8 & \textbf{60} & 28.5 & 0 & --- & --- \\
3000 & 15 & DirectedABC & 7 & 16.8 & \textbf{60} & 28.5 & 0 & --- & --- \\
3000 & 20 & DirectedABC & 7 & 16.8 & \textbf{60} & 28.5 & 0 & --- & --- \\
2000 & 10 & DirectedABC & 5 & 17.2 & \textbf{60} & 30.2 & 0 & --- & --- \\
2000 & 15 & DirectedABC & 5 & 17.2 & \textbf{60} & 30.2 & 0 & --- & --- \\
2000 & 20 & DirectedABC & 5 & 17.2 & \textbf{60} & 30.2 & 0 & --- & --- \\
3000 & 10 & DirectedABC & 5 & 17.2 & \textbf{60} & 30.2 & 0 & --- & --- \\
3000 & 15 & DirectedABC & 5 & 17.2 & \textbf{60} & 30.2 & 0 & --- & --- \\
3000 & 20 & DirectedABC & 5 & 17.2 & \textbf{60} & 30.2 & 0 & --- & --- \\
2000 & 20 & DirectedABC & 3 & 24.1 & \textbf{60} & 34.5 & 0 & --- & --- \\
2500 & 10 & DirectedABC & 3 & 24.1 & \textbf{60} & 34.5 & 0 & --- & --- \\
2500 & 15 & DirectedABC & 3 & 24.1 & \textbf{60} & 34.5 & 0 & --- & --- \\
2500 & 20 & DirectedABC & 3 & 24.1 & \textbf{60} & 34.5 & 0 & --- & --- \\
3000 & 10 & DirectedABC & 3 & 24.1 & \textbf{60} & 34.5 & 0 & --- & --- \\
3000 & 15 & DirectedABC & 3 & 24.1 & \textbf{60} & 34.5 & 0 & --- & --- \\
2000 & 10 & DirectedABC & 7 & 16.8 & \textbf{60} & 42.2 & 0 & --- & --- \\
2000 & 15 & DirectedABC & 7 & 16.8 & \textbf{60} & 42.2 & 0 & --- & --- \\
2000 & 20 & DirectedABC & 7 & 16.8 & \textbf{60} & 42.2 & 0 & --- & --- \\
2500 & 10 & DirectedABC & 5 & 17.2 & \textbf{60} & 44.0 & 0 & --- & --- \\
2500 & 15 & DirectedABC & 5 & 17.2 & \textbf{60} & 44.0 & 0 & --- & --- \\
2500 & 20 & DirectedABC & 5 & 17.2 & \textbf{60} & 44.0 & 0 & --- & --- \\
2000 & 10 & DirectedABC & 3 & 24.1 & \textbf{60} & 47.9 & 0 & --- & --- \\
2000 & 15 & DirectedABC & 3 & 24.1 & \textbf{60} & 47.9 & 0 & --- & --- \\
3000 & 20 & DirectedABC & 3 & 24.1 & \textbf{60} & 47.9 & 0 & --- & --- \\
3000 & 15 & ModifiedABC & 3 & --- & --- & --- & --- & 10.8 & 0 \\
2000 & 20 & ModifiedABC & 3 & --- & --- & --- & --- & 12.0 & 0 \\
2000 & 10 & ModifiedABC & 3 & --- & --- & --- & --- & 14.9 & 0 \\
2500 & 10 & ModifiedABC & 3 & --- & --- & --- & --- & 14.9 & 0 \\
2500 & 15 & ModifiedABC & 3 & --- & --- & --- & --- & 14.9 & 0 \\
2500 & 20 & ModifiedABC & 3 & --- & --- & --- & --- & 14.9 & 0 \\
2000 & 15 & ModifiedABC & 3 & --- & --- & --- & --- & 24.7 & 0 \\
3000 & 10 & ModifiedABC & 3 & --- & --- & --- & --- & 24.7 & 0 \\
3000 & 20 & ModifiedABC & 3 & --- & --- & --- & --- & 24.7 & 0 \\
2000 & 20 & StandardABC & 7 & --- & --- & 31.1 & 0 & --- & --- \\
2500 & 10 & StandardABC & 7 & --- & --- & 31.1 & 0 & --- & --- \\
2500 & 15 & StandardABC & 7 & --- & --- & 31.1 & 0 & --- & --- \\
2500 & 20 & StandardABC & 7 & --- & --- & 31.1 & 0 & --- & --- \\
3000 & 15 & StandardABC & 7 & --- & --- & 31.1 & 0 & --- & --- \\
3000 & 20 & StandardABC & 7 & --- & --- & 31.1 & 0 & --- & --- \\
2000 & 10 & StandardABC & 5 & --- & --- & 32.6 & 0 & --- & --- \\
2000 & 15 & StandardABC & 5 & --- & --- & 32.6 & 0 & --- & --- \\
2000 & 20 & StandardABC & 5 & --- & --- & 32.6 & 0 & --- & --- \\
2500 & 10 & StandardABC & 5 & --- & --- & 32.6 & 0 & --- & --- \\
2500 & 15 & StandardABC & 5 & --- & --- & 32.6 & 0 & --- & --- \\
2500 & 20 & StandardABC & 5 & --- & --- & 32.6 & 0 & --- & --- \\
3000 & 10 & StandardABC & 5 & --- & --- & 32.6 & 0 & --- & --- \\
3000 & 15 & StandardABC & 5 & --- & --- & 32.6 & 0 & --- & --- \\
3000 & 20 & StandardABC & 5 & --- & --- & 32.6 & 0 & --- & --- \\
2000 & 10 & StandardABC & 3 & --- & --- & 35.3 & 0 & --- & --- \\
2000 & 15 & StandardABC & 3 & --- & --- & 35.3 & 0 & --- & --- \\
2000 & 20 & StandardABC & 3 & --- & --- & 35.3 & 0 & --- & --- \\
2500 & 20 & StandardABC & 3 & --- & --- & 35.3 & 0 & --- & --- \\
3000 & 10 & StandardABC & 3 & --- & --- & 35.3 & 0 & --- & --- \\
3000 & 20 & StandardABC & 3 & --- & --- & 35.3 & 0 & --- & --- \\
2000 & 10 & StandardABC & 7 & --- & --- & 44.8 & 0 & --- & --- \\
2000 & 15 & StandardABC & 7 & --- & --- & 44.8 & 0 & --- & --- \\
3000 & 10 & StandardABC & 7 & --- & --- & 44.8 & 0 & --- & --- \\
2500 & 10 & StandardABC & 3 & --- & --- & 46.7 & 0 & --- & --- \\
2500 & 15 & StandardABC & 3 & --- & --- & 46.7 & 0 & --- & --- \\
3000 & 15 & StandardABC & 3 & --- & --- & 46.7 & 0 & --- & --- \\
\hline
\end{longtable}
As we can see from the table, the only algorithm that reaches the thresholds in the first class is the "DirectedABC", while the "ModifiedABC" algorithm is the only one that reaches the thresholds in the second and third class. From the critical difference diagrams, it can be observed that the "DirectedABC" algorithm is statistically significantly better than the "ModifiedABC" algorithm on the first class, while the "ModifiedABC" algorithm is statistically significantly better than the "DirectedABC" algorithm on the second and third class.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 1}
        \includegraphics[width=0.66\textwidth]{imgs/abc/step3/1_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 2}
        \includegraphics[width=0.66\textwidth]{imgs/abc/step3/2_critical_difference.png}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \caption{Class 3}
        \includegraphics[width=0.66\textwidth]{imgs/abc/step3/3_critical_difference.png}
    \end{subfigure}
    \caption{Critical Difference diagrams for ABC algorithms after the second filtering step}
    \label{fig:cd3}
\end{figure}

\subsection{Final Comparison}
Now that the best algorithms of each class and family are identified, a more detailed comparison can be made between them. In the next subsections the best algorithms of each family are compared with each other, discussing the differences in their performance and the influence of their parameters on the different problems.
\subsubsection{First class}
The problems of the first class where the algorithms are tested are the 7th and the 8th. In the following table \ref{tab:best_algorithms_class1} the results for each family are reported.
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|c|c||c|ccccc|}
\hline
\multicolumn{2}{|c||}{C1} & \textbf{Algorithm} & \textbf{MeanError} & \textbf{MedianError} & \textbf{StdError} & \textbf{MeanSE} & \textbf{Thresholds} \\
\hline

\multirow{4}{*}{\rotatebox{90}{\textbf{Problems}}}

& \multirow{2}{*}{7} 
& ABC & $8.93\times 10^{-6}$ & $1.52\times 10^{-5}$ & $8.93\times 10^{-6}$ & $8.93\times 10^{-7}$ & 0 \\

& 
& PSO & $1.14\times 10^{-13}$ & $1.14\times 10^{-13}$ & 0.0 & 0.0 & 100 \\

\cline{2-8}

& \multirow{2}{*}{8} 
& ABC & 0.1765 & 0.1779 & 0.1817 & 0.0018 & 0 \\

& 
& PSO & 0.0382 & 0.0 & 0.1364 & 0.0136 & 79 \\

\hline
\end{tabular}
\caption{Performance comparison by problem}
\label{tab:best_algorithms_class1}
\end{table}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/plot/convergence_summary_comparison7.png}
        \caption{Convergence plot on problem 7}
        \label{fig:p7_convergence}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/plot/convergence_summary_comparison8.png}
        \caption{Convergence plot on problem 8}
        \label{fig:p8_convergence}
    \end{subfigure}
    \caption{Convergence comparison on class 1 problems}
    \label{fig:class1_comparison}
\end{figure}
\noindent From the table and the plots \ref{fig:class1_comparison} it can be observed that the PSO algorithm is much better than the ABC algorithm on the first class of problems: it reaches all the thresholds in the first problems, and 79 out of 100 in the second problem. The ABC algorithm does not reach any threshold in both problems, and it has a much higher mean and variance of the error. The convergence plots also show that the PSO algorithm converges much faster than the ABC algorithm.
\\[1em]
Although, it must be noted that this does not come unexpected. The parameters of the PSO is more explorative than the ABC algorithm, and thus it benefits from the problems structure.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/Figures/f7.jpg}
        \caption{Problem 7 shape on the first two dimensions}
        \label{fig:problem7}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/Figures/f8.jpg}
        \caption{Problem 8 shape on the first two dimensions}
        \label{fig:problem8}
    \end{subfigure}
    \caption{Class 1 problems shapes}
    \label{fig:problem78}
\end{figure}
\noindent Indeed from the figures in \ref{fig:problem78} it can be observed that the problems of the first class have a structure exploitable by the PSO algorithm. On the other hand, the ABC algorithm will slowly find better solutions, since the bees have to first randomly find a good solution. Once a good solution is found, the bees will randomly explore around it, thus slowly improving the solution. The PSO algorithm particles are instead able to quickly exploit the problem thanks to the solutions found by the neighbors, thus quickly converging to the optimal solution.
\\[1em]
The Wilcoxon test confirms that the PSO algorithm is statistically significantly better than the ABC algorithm on the first class of problems, with $p$-values $3.956\times 10^{-18}$ and $3.24\times 10^{-13}$ for the first and the second problem respectively. Of course, the first $p$-value is far lower than the second one, this is expected due to their different mean errors on the two problems.
\subsubsection{Second class}
The second class problems on which ABC and PSO are compared are the 15th and the 16th. The results are reported in table \ref{tab:best_algorithms_class2}, and the convergence plots are shown in figure \ref{fig:class2_comparison}.
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|c|c||c|ccccc|}
\hline
\multicolumn{2}{|c||}{C2} & \textbf{Algorithm} & \textbf{MeanError} & \textbf{MedianError} & \textbf{StdError} & \textbf{MeanSE} & \textbf{Thresholds} \\
\hline

\multirow{4}{*}{\rotatebox{90}{\textbf{Problems}}}

& \multirow{2}{*}{15} 
& ABC & $4.883\times10^8$ & $4.738\times10^8$ & $1.672\times10^8$ & $1.672\times10^7$ & $0$ \\

& 
& PSO & $9.894\times10^5$ & $3.086\times10^5$ & $1.747\times10^6$ & $1.747\times10^5$ & $0$ \\

\cline{2-8}

& \multirow{2}{*}{16} 
& ABC & $1.66$ & $0.14$ & $3.7$ & $0.37$ & $0$ \\

& 
& PSO & $22.33$ & $10.5$ & $24$ & $2.4$ & $3$ \\

\hline
\end{tabular}
\caption{Performance comparison by problem}
\label{tab:best_algorithms_class2}
\end{table}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/plot/convergence_summary_comparison15.png}
        \caption{Convergence plot on problem 15}
        \label{fig:p15_convergence}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/plot/convergence_summary_comparison16.png}
        \caption{Convergence plot on problem 16}
        \label{fig:p16_convergence}
    \end{subfigure}
    \caption{Convergence comparison on class 2 problems}
    \label{fig:class2_comparison}
\end{figure}
\noindent In this case the competition is more balanced. In the first problem, the PSO algorithm has a much lower mean error than the ABC algorithm, although they both do not reach any threshold. Compared to the previous class, the PSO algorithm is not robust: the standard deviation is even higher than the mean, thus making it very random. The ABC algorithm is instead more robust, with a lower standard deviation than the mean, although it is still very high. For this reason, the PSO algorithms performs better than the ABC algorithm in terms of mean error, but is less robust.
\\[1em]
\noindent In the second problem, the ABC algorithm finally takes the upper hand, with a much lower mean error than the PSO algorithm. Surprisingly the PSO algorithm reaches 3 thresholds, while the ABC algorithm does not reach any threshold. This is explained by the standard deviation and the median: the first one is high, even a bit more than the mean error. Meanwhile, the median is almost half the mean error. This means that the results are really sparse, with some runs that find very good solutions, while others that find very bad solutions. The ABC algorithm is even less robust, with a much higher standard deviation than the mean error, thus making it even more random. But comparing it with the PSO algorithm, it performs better, even though it does not reach any threshold.
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/Figures/f15.jpg}
        \caption{Problem 15 shape on the first two dimensions}
        \label{fig:problem15}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/Figures/f16.jpg}
        \caption{Problem 16 shape on the first two dimensions}
        \label{fig:problem16}
    \end{subfigure}
    \caption{Class 2 problems shapes}
    \label{fig:problem1516}
\end{figure}
\noindent From the figures in \ref{fig:problem1516} it can be observed that the problems of the second class are more complex than the ones of the first class. But the first one is still suitable for the PSO algorithm, since it has a structure that can be exploited by the particles. Some particles may fall in one of the steps, but since it is a network structure, they can quickly know where to find better solutions thanks to the neighbors that surpassed the little step where the previous fell. However, this is one of the worst case for the bees, where each step may become a source that has to be completely exploited and abandoned after it reaches the maximum limit before looking for another solution. 
\\[1em]
The second problem is the perfect opposite of the first one. It is a very rugged problem, with many local minima, thus making it very hard for the PSO algorithm to find good solutions. The swarm may fall on a local minimum and take time to escape. Also, the inertia plays a crucial role here, if the particles move fastly enough, they may be able to escape the local minima. But the ABC is able to manage this, since the bees leave a solution after a while. 
\\[1em]
This is where the ABC algorithm is able to outperform the PSO algorithm, and will become clear in the last class problems. Since now, the variables that made the problem instances were merged with linear relations. But in the last class, they are putted together with many non-linear relations. This makes the problem more rugged and harder for the PSO.
\subsubsection{Third class}
\section{Conclusion}

\printbibliography

\end{document}