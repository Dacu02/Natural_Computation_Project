\documentclass[11pt]{article}

% \usepackage[utf-8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{setspace}
\usepackage[style=numeric, backend=bibtex]{biblatex}
\addbibresource{Documentation.bib}
\usepackage{xcolor}
\pagecolor[rgb]{0.05,0.05,0.05}
\color[rgb]{0.9,0.9,0.9}

\geometry{margin=1in}
\onehalfspacing

\title{Natural Computation Project Documentation}
\author{Noemi Biancamano \and Davide D'Acunto}
\date{}

\begin{document}

\maketitle

\begin{abstract}
\noindent The following document compares the performance of different algorithms implemented for solving problems instances generated by the Generalized Numerical Benchmark Generator (GNBG). Two main families of the swarm intelligence algorithms are considered: Particle Swarm Optimization (PSO) and Artificial Bee Colony (ABC).
\end{abstract}
\tableofcontents
\newpage
\section{Introduction}
\subsection{Generalized Numerical Benchmark Generator}
The Generalized Numerical Benchmark Generator (GNBG) is a set of benchmark functions used for evaluating the performance of optimization algorithms. These 24 functions are grouped into three categories of 8 problems each:
\begin{itemize}
    \item \textbf{Unimodal functions}: these functions have a single global minimum, often having a landscape smoother than the other categories and sometimes even convex
    \begin{itemize}
        \item This class evalues how well the algorithm obtains information from the landscape, and how well it exploits this information to converge to the global minimum, since in this case there are not local minima to deceive the algorithm.
    \end{itemize}
    \item \textbf{Single component Multimodal functions}: these functions have multiple local minima, making them more challenging for optimization algorithms since their ability to deceive the algorithms
    \begin{itemize}
        \item This class evaluates how well the algorithm can escape from local minima, and how well it can explore the search space to find the global minimum.
    \end{itemize}
    \item \textbf{Multi component Multimodal functions}: these functions have multiple local minima, but they are more complex than the single component multimodal functions, since they are composed of multiple components, which make the landscape unpredictable and more difficult to navigate
    \begin{itemize}
        \item This class evaluates how well the algorithm can navigate through a complex landscape, and how well it can balance exploration and exploitation to find the global minimum.
    \end{itemize}
    \end{itemize}
\subsection{Particle Swarm Optimization}
The Particle Swarm Optimization (PSO) is a population-based optimization in which a swarm of particles (potential solutions) moves through the search space to find the optimal solution. Each particle adjusts its position based on its own experience and the experience of neighboring particles.
\\[1em]
The update rule for the velocity and position of the $i$-th particle on iteration $t$ in PSO with inertia weight \cite{pso} is given by:
\begin{align*}
\overset{\rightarrow}{v_i}(t+1) &= w \cdot \overset{\rightarrow}{v_i}(t) + \mathcal U(0,c_1) \cdot (\overset{\rightarrow}{p_i} - \overset{\rightarrow}{x_i}(t)) + \mathcal U(0,c_2) \cdot (\overset{\rightarrow}{g_i} - \overset{\rightarrow}{x_i}(t)) \\
\overset{\rightarrow}{x_i}(t+1) &= \overset{\rightarrow}{x_i}(t) + \overset{\rightarrow}{v_i}(t+1)
\end{align*}
In the subsequent sections, each term of the update rule will be explained in detail, along with the parameters involved and their influence on the algorithm's performance.
\subsubsection{Population \& Topology}
A higher population size allows for a more extensive exploration of the search space, but decreases the number of iterations, meanwhile a smaller population size allows for more iterations on fewer particles. The topology affects how particles share information: it oscillates between a global topology, where all particles connected between each other, and a local topology, where particles only interact with few neighbors. 
\\[1em]
The topologies considered in this project are:
\begin{itemize}
    \item \textbf{Random Topology}: particles are connected randomly to $k$ other particles, where $k$ is set to 5 in the experiments. 
    \item \textbf{Star Topology}: all particles are connected to a central particle, which shares the best position to all other particles.
    \item \textbf{Torus Topology}: this topology is a special case of the Von Neumann topology, in which particles are arranged in a 2D grid, and each particle is connected to its four immediate neighbors (up, down, left, right). The Torus topology wraps around, meaning that the particles on the edges are connected to those on the opposite edge, creating a toroidal structure.
\end{itemize}
Since a Torus topology is adopted, the chosen population sizes will be perfect squares, in order to create a square grid of particles. The population sizes considered in the experiments are $121$ and $169$. 
\subsubsection{Position \& Velocity}
The position of each particle represents a potential solution to the problem, while the velocity determines how the particle moves through the search space, with respect to the other parameters. Since the solution space is not $\mathbb R^d$, they must be limited to the range of the problem:
\begin{itemize}
    \item \textbf{Position}: the position of each particle is limited to the range of the problem, which is $[-100, 100]$ for all dimensions. If a particle is going to exceed a boundary, its velocity is shrunk by a factor such that the particle falls on the boundary.
    \item \textbf{Velocity}: the velocity of each particle is limited to a certain range, if the velocity exceeds the maximum (or minimum) velocity, it is clamped to the maximum (or minimum) velocity. The range of the velocity is set to $\pm0.15$ times the difference between the maximum and minimum positions: $\left[-30, 30\right]$. It is important to note that the velocity is independently limited for each dimension. This customization was suggested in \cite{clamp} 
\end{itemize}
\subsubsection{Cognitive and Social Weights}
The cognitive and social weights influence respectively the particle's tendency to return to its own best position and the tendency to move towards the best position found by its neighbors. A higher cognitive weight encourages particles to explore the search space more independently, while a higher social weight encourages particles to converge towards the best solution found by the swarm. The values of the cognitive and social weights considered are all combinations of $c_1$ and $c_2$ in $\{0.75, 1.25\}$.
\subsubsection{Inertia}
The inertia weight controls the influence of the previous velocity on the current velocity. It is somewhat similar to the temperature of the particles. In this case other than the standard fixed inertia weight, a linearly decreasing inertia weight is also considered. The inertia weights start from $0.9$ or $0.7$ and eventually if enabled decreases to $0.4$ through the iterations.

\subsection{Artificial Bee Colony}

\section{Methods}
In order to compare the performance of the algorithms, statistical tests are performed on the results obtained from the experiments ran on a subset of the problem instances. Based on the type of comparison, i.e. whether many algorithms of the same family are compared among themselves, or whether one algorithm from each family is compared, different tests are performed.
\\[1em]
The methods used for the comparisons will be explained in detail in the following subsections, along with the reason behind their choice. In the next section then the implementation of the methods will be swiftly explained, and finally their planning and execution will be described. The notions are taken from \cite{demsar}.
\subsection{Multiple algorithms of one family comparison}
This case considers the comparison of multiple algorithms of the same family: algorithms belonging to the same family (PSO or ABC) but differing in the parameters used. In this case it is assumed that the algorithms are more than two, and that the results obtained from the experiments are not normally distributed. For this reason, non-parametric tests are performed.
\subsubsection{Friedman Test}
The Friedman test is among the most widely used non-parametric tests for comparing multiple algorithms on multiple datasets, where in this case the dataset consists of the results obtained from the experiments. Its first step is to rank the algorithms on each problem, assigning the rank $1$ to the best performing algorithm, $2$ to the second best, and so on. If two or more algorithms have the same performance, they are assigned the average rank (e.g., if two algorithms are tied for first place, they would both receive a rank of $\dfrac{1+2}2=1.5$).
\\[1em]
After ranking the algorithms on each problem, the mean rank of each algorithm across the considered problems is calculated. This mean rank allows to determine an estimator of the performance of each algorithm which is not affected by the different scales of the results obtained from the different problems. In order to determine whether the observed differences in mean ranks are statistically significant, the Friedman test statistic is calculated: $$Q=\dfrac{12n}{k(k+1)}\sum_{j=1}^{k}\left(\bar r_j-\dfrac{k+1}{2}\right)^2,\qquad \bar r_j=\dfrac1n\sum_{i=1}^n r_{ij},\qquad Q\sim\chi_{(k-1)}^2$$
In the formula, $n$ is the number of problems, $k$ is the number of algorithms, and $\bar r_j$ is the mean rank of the $j$-th algorithm. In order to understand the formula, it is important to explain the intuition behind it. First, the rank $r_{ij}$ itself is distributed accordingly to a discrete uniform distribution. Since under the null hypothesis, all algorithms are expected to perform equally, so they share the same mean rank: $$r_{ij}\sim\mathcal U\left\{1,\ldots,k\right\},\quad \mathbb E\left[r_{ij}\right]=\dfrac{k+1}{2}\overset{H_0}=\mathbb E\left[\bar r_j\right],\quad Var\left[r_{ij}\right]=\dfrac{k^2-1}{12}$$
From which the variance of the mean ranks can be derived: $$Var\left[\bar r_j\right]=Var\left[\dfrac1n\sum_{i=1}^n r_{ij}\right]=\dfrac 1{n^2}\cdot n\dfrac{k^2-1}{12}=\dfrac{k^2-1}{12n}$$
Now, the Friedman test purpose is to measure the deviation of the observed mean ranks from the expected mean rank under the null hypothesis: $$S=\sum_{j=1}^k\left(\bar r_j-\dfrac{k+1}2\right)^2$$
This deviation must then be normalized by the rank variance, in order to be comparable. The Friedman test statistic also introduces one degree of freedom correction because of the covariance between the ranks. In the end, the formula takes the form of a chi-square distribution with $k-1$ degrees of freedom, as shown above. 
\\[1em]
The more the observed mean ranks deviate from the expected mean rank, the higher the value of $Q$, and the more likely it is to reject the null hypothesis. Also, $k$ and $n$ have an influence on the value of $Q$ too: as the number of algorithms $k$ increases, the statistics decreases, while if the number of experiments $n$ increases, the statistic increases.
\subsubsection{Nemenyi post-hoc}
The Friedman test determines whether there are statistically significant differences among the algorithms, but it doesn't specify which algorithms differ. In order to retrieve this information, the Nemenyi post-hoc test is performed. This test purpose is to compare all pairs of algorithms, and determine whether the null hypothesis of equal performance can be rejected for each pair. 
\\[1em]
In order to determine whether the performance of two algorithms $i$ and $j$ is significantly different, the variance of the difference between their mean ranks is calculated: $$Var\left[\bar r_i-\bar r_j\right]=Var\left[\bar r_i\right]+Var\left[\bar r_j\right]-2\ Cov\left[\bar r_i,\bar r_j\right]$$
As aforementioned, the ranks are not independent between themselves: if one algorithm has a high rank on a problem, another algorithm must have a low rank on the same problem. In order to retrieve the covariance between the ranks, some steps must be taken. Knowing that the sum of the ranks on each problem is constant: $$\sum_{j=1}^k r_{ij} = \frac{k(k+1)}{2}\Longrightarrow0=Var\left[\sum_{j=1}^k r_{ij}\right]=\sum_{j=1}^kVar\left[r_{ij}\right]+2\sum_{1\le j<\ell\le k}Cov(r_{ij},r_{i\ell}),\qquad\forall i$$
Solving for the covariance, knowing the variance and knowing that all the covariances are equal, it can be derived that: $$Cov(r_{ij},r_{i\ell})=-\dfrac{k+1}{12},\qquad 1\le j<\ell\le k,\quad\forall i$$
From which the variance of the difference between the mean ranks can be derived: $$Var\left[\bar r_i-\bar r_j\right]=\dfrac{k^2-1}{12n}+\dfrac{k^2-1}{12n}+2\cdot\dfrac{k+1}{12n}=\dfrac{k(k+1)}{6n}$$
This would be sufficient to perform a z-test on the difference between the mean ranks of the two algorithms, but since multiple comparisons are performed, a correction is done by introducing the critical value of the Studentized range distribution $q_\alpha$, which is defined as: $$q=\dfrac{\max\left(Z_i\right)-\min\left(Z_i\right)}{SE\left(Z_i\right)}\qquad q\sim q_{k,\nu}$$
Where $k$ is the number of algorithms, $\nu$ is the degrees of freedom. Since the variances of the mean ranks are known, it is assumed that $\nu=\infty$, meanwhile for the same reason mentioned before, one degree of freedom is detracted from $k$, so the critical value is $q_\alpha=q_{k-1,\infty}(\alpha)$.
\\[1em]
In the end, the performance of two algorithms $i$ and $j$ is significantly different if the absolute difference between their mean ranks is greater than the critical difference: $$|\bar r_i-\bar r_j|>q_\alpha\sqrt{Var\left[\bar r_i-\bar r_j\right]}=q_\alpha\sqrt{\dfrac{k(k+1)}{6n}}=CD$$
This quantity is called the critical difference (CD), and it represents the minimum difference between the mean ranks of two algorithms for their performance to be considered significantly different. As the number of algorithms $k$ increases, the critical difference increases, so it is harder to find significant differences between the algorithms. On the other hand, as the number of problems $n$ increases, the critical difference decreases. The $\alpha$ level also has an influence on the critical difference: as $\alpha$ decreases, the critical value $q_\alpha$ increases.
\subsection{One algorithm for each family comparison}
This case considers the comparison of one algorithm from each family: one PSO and one ABC. In this case, it is assumed that the results obtained from the experiments are not normally distributed. For this reason, non-parametric tests are performed.
\noindent \\
In the next sections, the Wilcoxon test is performed to compare the performance of the two algorithms on each problem, and then a post-hoc comparison is performed to determine whether the observed differences are statistically significant. Its used as an alternative to the paired t-test when the normality assumption is not met.
\subsubsection{Wilcoxon Test}
The Wilcoxon signed-rank test is a non-parametric test used to compare two related samples, in this case the results obtained from the two algorithms on the same problem instances. The test is based on the differences between the paired observations, and it assesses whether the median of these differences is significantly different from zero. 
\\
Considering two algorithms $A$ and $B$, the first step is to calculate the differences between the paired observations: 
$$d_i = A_i - B_i$$ 
where $A_i$ and $B_i$ are the results obtained from the two algorithms on the $i$-th problem instance. Then, the absolute values of these differences are ranked, ignoring any differences that are equal to zero. If there are ties in the absolute differences, they are assigned the average rank(as Friedman ranking). Next, the ranks are assigned a positive or negative sign based on the original differences: if $d_i$ is positive, the rank is positive; if $d_i$ is negative, the rank is negative. 
Under the null hypothesis that there is no difference between the two algorithms, the sum of the positive ranks and the sum of the negative ranks should be approximately equal. The test statistic is then calculated as the smaller of these two sums:
$$W = \min\left(\sum_{d_i > 0} r_i + \dfrac{1}{2} \sum_{d_i = 0} r_i, \sum_{d_i < 0} r_i + \dfrac{1}{2} \sum_{d_i = 0} r_i\right)$$
Where $r_i$ is the rank of the absolute difference $|d_i|$, note that ranks of $d_i = 0$ are split equally between the positive and negative sums. \\
The p-value can be computed considering that under the null hypothesis, all the configuration are equally likely, with probability $\dfrac{1}{2^n}$, where $n$ is the number of non-zero differences. 
In order to obtain the probability we consider all the configurations of signs for the ranks, and we count how many of them lead to a sum of positive ranks (or negative ranks) less than or equal to the observed test statistic $W$. The p-value is then calculated as:
$$p = \dfrac{\text{number of configurations with } W' \leq W}{2^n}$$
Where $W'$ is the test statistic calculated for each configuration of signs. If the p-value is less than the chosen significance level $\alpha$, we reject the null hypothesis, indicating that there is a statistically significant difference between the two algorithms. 
For larger sample sizes its demonded that the distribution of the test statistic $W$ can be approximated by a normal distribution, allowing for the use of a z-test to calculate the p-value. In this case, the test statistic is standardized as follows:
$$Z = \dfrac{W - E[W^+]}{\sqrt{Var(W^+)}}$$
Where $E[W^+]$ and $Var(W^+)$ are the mean and variance of the test
statistic $W$ under the null hypothesis, which can be calculated as:
$$E[W^+] = \dfrac{n(n+1)}{4},\quad Var(W^+) = \dfrac{n(n+1)(2n+1)}{24}$$
The p-value can then be calculated using the standard normal distribution.



\subsubsection{Comparison post-hoc}

\section{Execution}
In the following section, the execution procedure is thoroughly explained, along with the machines used for the experiments.
\\[1em]
Regarding the machines, the experiments were run on the High Performance Computing (HPC) cluster of the University of Salerno, which is composed of $16$ nodes executing Ubuntu Linux $6.8$ mounting each $72$ CPU cores, for a total of $1152$ cores. For this reason, the experiments were parallelized: for each node a process is executed, each one initializes $72$ threads, so that each thread runs an experiment on a different problem instance. In order to distribute the load among the nodes, the experiments are distributed with a job array. It allows to submit a single job to the cluster, which will then replicate itself on each node, but with a different index, so that each node will execute a different subset of the experiments. The results obtained from the experiments are then collected, aggregated, and then analyzed with the statistical tests.
\\[1em]
In order to take into account the maximum execution time of the experiments on the nodes allowed by the cluster policies, the mean time to execute an experiment is estimated to last less than 20 minutes. Since the number of cpus is $1152$, and the maximum time allowed for a job is $9$ hours, the maximum amount of experiments that can be executed in parallel is $1152\cdot\dfrac{9\cdot60}{20}=31104$. This number far exceeds the amount of experiments that will be executed, and in order to avoid overloading the cluster, the experiments are distributed through time on fewer nodes. 
\subsection{Framework}
The framework of the project is implemented in Python, and it is composed of various modules that represent the different components of the algorithms, which were developed independently from each other, in order to split the execution part from the data retrieval and analysis part.
\subsubsection{\texttt{Framework.py}}
\subsubsection{\texttt{Algorithm.py}}
Through this module, the abstract class that an algorithm must implement is defined. This way by managing the algorithms through a common interface, the execution of the experiments is simplified, and the results can be easily retrieved and analyzed by defining a common format for the results. This module also defines the common parameters of the algorithms, such as the population size, the verbosity level, the number of generations and the seed.
\subsubsection{\texttt{ParticleSwarmOptimization.py}}
The Particle Swarm Optimization (PSO) algorithm is implemented in this module through the Pyswarms library \cite{pyswarms} and the \texttt{Algorithm} abstract class. This library was chosen because it provided a flexible implementation of the PSO algorithm, which exposed the parameters needed for the experiments, the linear decay of the parameters, and the possibility to implement custom topologies. 
\\[1em]
The first part of the file contains literals describing:
\begin{itemize}
    \item The strategy to adopt when a particle is going to exceed the boundaries of the problem
    \item The strategy to adopt when a particle is going to exceed the velocity limits
    \item The strategy to vary the parameters of the algorithm through the iterations
    \item A custom implementation of the Torus topology, which is not provided by the library
\end{itemize}
\subsubsection{\texttt{ArtificialBeeColony.py}}
\subsubsection{\texttt{StatComparison.py}}
\subsection{Experiments}
\subsubsection{PSO}
\subsubsection{ABC}
\subsection{Planning}

\section{Results discussion}
\subsection{PSO}
\subsection{ABC}
\subsection{PSO vs ABC}

\section{Conclusion}

\printbibliography

\end{document}