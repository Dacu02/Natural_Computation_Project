\documentclass[11pt]{article}

% \usepackage[utf-8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{setspace}
\usepackage[style=numeric, backend=bibtex]{biblatex}
\addbibresource{Documentation.bib}
\usepackage{xcolor}
\pagecolor[rgb]{0.05,0.05,0.05}
\color[rgb]{0.9,0.9,0.9}

\geometry{margin=1in}
\onehalfspacing

\title{Natural Computation Project Documentation}
\author{Noemi Biancamano \and Davide D'Acunto}
\date{}

\begin{document}

\maketitle

\begin{abstract}
\noindent The following document compares the performance of different algorithms implemented for solving problems instances generated by the Generalized Numerical Benchmark Generator (GNBG). Two main families of the swarm intelligence algorithms are considered: Particle Swarm Optimization (PSO) and Artificial Bee Colony (ABC).
\end{abstract}
\tableofcontents
\newpage
\section{Introduction}
\subsection{Generalized Numerical Benchmark Generator}
\subsection{Particle Swarm Optimization}
The Particle Swarm Optimization (PSO) is a population-based optimization in which a swarm of particles (potential solutions) moves through the search space to find the optimal solution. Each particle adjusts its position based on its own experience and the experience of neighboring particles.
\\[1em]
The update rule for the velocity and position of the $i$-th particle on iteration $t$ is given by:
\begin{align*}
\overset{\rightarrow}{v_i}(t+1) &= w \cdot \overset{\rightarrow}{v_i}(t) + \mathcal U(0,c_1) \cdot (\overset{\rightarrow}{p_i} - \overset{\rightarrow}{x_i}(t)) + \mathcal U(0,c_2) \cdot (\overset{\rightarrow}{g_i} - \overset{\rightarrow}{x_i}(t)) \\
\overset{\rightarrow}{x_i}(t+1) &= \overset{\rightarrow}{x_i}(t) + \overset{\rightarrow}{v_i}(t+1)
\end{align*}
In the subsequent sections, each term of the update rule will be explained in detail, along with the parameters involved and their influence on the algorithm's performance.
\subsubsection{Population \& Topology}
A higher population size allows for a more extensive exploration of the search space, but decreases the number of iterations, meanwhile a smaller population size allows for more iterations on fewer particles. The topology affects how particles share information: it oscillates between a global topology, where all particles are neighbors, and a local topology, where particles only interact with a subset of the swarm.
\\[1em]
The topologies considered in this project are:
\begin{itemize}
    \item \textbf{Random Topology}: particles are connected randomly to $k$ other particles, where $k$ is set to 5 in the experiments. 
    \item \textbf{Star Topology}: all particles are connected to a central particle, which shares the best position to all other particles.
    \item \textbf{Torus Topology}: this topology is a special case of the Von Neumann topology, in which particles are arranged in a 2D grid, and each particle is connected to its four immediate neighbors (up, down, left, right). The Torus topology wraps around, meaning that the particles on the edges are connected to those on the opposite edge, creating a toroidal structure.
\end{itemize}
Since a Torus topology is adopted, the chosen population sizes will be perfect squares, in order to create a square grid of particles. The population sizes considered in the experiments are $121$ and $169$. 
\subsubsection{Position \& Velocity}
The position of each particle represents a potential solution to the problem, while the velocity determines how the particle moves through the search space, with respect to the other parameters. Since the solution space is not $\mathbb R^d$, they must be limited to the range of the problem:
\begin{itemize}
    \item \textbf{Position}: the position of each particle is limited to the range of the problem, which is $[-100, 100]$ for all dimensions. If a particle is going to exceed a boundary, its velocity is shrunk by a factor such that the particle falls on the boundary.
    \item \textbf{Velocity}: the velocity of each particle is limited to a certain range, if the velocity exceeds the maximum (or minimum) velocity, it is clamped to the maximum (or minimum) velocity. The range of the velocity is set to $\pm0.15$ times the difference between the maximum and minimum positions: $\left[-30, 30\right]$. It is important to note that the velocity is independently limited for each dimension. This customization was suggested in \cite{clamp} 
\end{itemize}
\subsubsection{Cognitive and Social Weights}
\subsubsection{Inertia}

\subsection{Artificial Bee Colony}

\section{Methods}
In order to compare the performance of the algorithms, statistical tests are performed on the results obtained from the experiments ran on a subset of the problem instances. Based on the type of comparison, i.e. whether many algorithms of the same family are compared among themselves, or whether one algorithm from each family is compared, different tests are performed.
\\[1em]
The methods used for the comparisons will be explained in detail in the following subsections, along with the reason behind their choice. In the next section then the implementation of the methods will be swiftly explained, and finally their planning and execution will be described. 
\subsection{Multiple algorithms of one family comparison}
This case considers the comparison of multiple algorithms of the same family: algorithms belonging to the same family (PSO or ABC) but differing in the parameters used. In this case it is assumed that the algorithms are more than two, and that the results obtained from the experiments are not normally distributed. For this reason, non-parametric tests are performed.
\subsubsection{Friedman Test}
The Friedman test is among the most widely used non-parametric tests for comparing multiple algorithms on multiple datasets, where in this case the dataset consists of the results obtained from the experiments. Its first step is to rank the algorithms on each problem, assigning the rank $1$ to the best performing algorithm, $2$ to the second best, and so on. If two or more algorithms have the same performance, they are assigned the average rank (e.g., if two algorithms are tied for first place, they would both receive a rank of $\dfrac{1+2}2=1.5$).
\\[1em]
After ranking the algorithms on each problem, the mean rank of each algorithm across the considered problems is calculated. This mean rank allows to determine an estimator of the performance of each algorithm which is not affected by the different scales of the results obtained from the different problems. In order to determine whether the observed differences in mean ranks are statistically significant, the Friedman test statistic is calculated: $$Q=\dfrac{12n}{k(k+1)}\sum_{j=1}^{k}\left(\bar r_j-\dfrac{k+1}{2}\right)^2,\qquad \bar r_j=\dfrac1n\sum_{i=1}^n r_{ij},\qquad Q\sim\chi_{(k-1)}^2$$
In the formula, $n$ is the number of problems, $k$ is the number of algorithms, and $\bar r_j$ is the mean rank of the $j$-th algorithm. In order to understand the formula, it is important to explain the intuition behind it. First, the rank $r_{ij}$ itself is distributed accordingly to a discrete uniform distribution. Since under the null hypothesis, all algorithms are expected to perform equally, so they share the same mean rank: $$r_{ij}\sim\mathcal U\left\{1,\ldots,k\right\},\quad \mathbb E\left[r_{ij}\right]=\dfrac{k+1}{2}\overset{H_0}=\mathbb E\left[\bar r_j\right],\quad Var\left[r_{ij}\right]=\dfrac{k^2-1}{12}$$
From which the variance of the mean ranks can be derived: $$Var\left[\bar r_j\right]=Var\left[\dfrac1n\sum_{i=1}^n r_{ij}\right]=\dfrac 1{n^2}\cdot n\dfrac{k^2-1}{12}=\dfrac{k^2-1}{12n}$$
Now, the Friedman test purpose is to measure the deviation of the observed mean ranks from the expected mean rank under the null hypothesis: $$S=\sum_{j=1}^k\left(\bar r_j-\dfrac{k+1}2\right)^2$$
This deviation must then be normalized by its own variance, in order to be comparable. The Friedman test statistic also introduces one degree of freedom correction because of the covariance between the ranks (by knowing $n-1$ ranks, the last one is determined since the sum of the ranks is constant). In the end, the formula takes the form of a chi-square distribution with $k-1$ degrees of freedom, as shown above. 
\\[1em]
The more the observed mean ranks deviate from the expected mean rank, the higher the value of $Q$, and the more likely it is to reject the null hypothesis. Also, $k$ and $n$ have an influence on the value of $Q$ too: as the number of algorithms $k$ increases, the statistics decreases, while if the number of experiments $n$ increases, the statistic increases. 
\subsubsection{Nemenyi post-hoc}
\subsection{One algorithm for each family comparison}
\subsubsection{Wilcoxon Test}
\subsubsection{Comparison post-hoc}

\section{Execution}
\subsection{Framework}
\subsubsection{Algorithms}
\subsubsection{Problems}
\subsubsection{Tests}
\subsection{Experiments}
\subsubsection{PSO}
\subsubsection{ABC}
\subsection{Planning}

\section{Results discussion}
\subsection{PSO}
\subsection{ABC}
\subsection{PSO vs ABC}

\section{Conclusion}

\printbibliography

\end{document}